{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import datetime\n",
    "import time\n",
    "import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "import sklearn\n",
    "import dask\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "import fastparquet\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "# Percentiles\n",
    "from scipy.stats import percentileofscore\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventor mobility effects on citations at new location\n",
    "\n",
    "1.1 Find new location citations to inventor's old patents and inventor's new patents\n",
    "- Show new patents have greater rates of citation compared to old patents\n",
    "\n",
    "1.2 Show rate of citation at new location increases, but doesn't translate to more similar inventions\n",
    "1. Find new patent's citations in new location\n",
    "2. Find new patents' similarity to new citations\n",
    "3. Find new patents' similarity to old patents of same assignee who don't cite new patent\n",
    "\n",
    "### Find inventors who moved and their patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140076\n",
      "12846\n",
      "10826\n"
     ]
    }
   ],
   "source": [
    "yv = \"appyear\"\n",
    "# All inventors who have moved\n",
    "ip = pd.read_pickle(\"DataStore/2018-07/inv_move_pats_0712.pkl\")\n",
    "print(len(ip))\n",
    "\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\", \"inv_msa\", \"gyear\", \"appyear\"])\n",
    "\n",
    "# Add application year\n",
    "ip[yv] = ip[\"patent\"].map(dict(zip(pdf[\"patent\"], pdf[yv])))\n",
    "\n",
    "# Sort by inventor, grant year\n",
    "ip = ip.sort_values([\"inventor_id\", yv])\n",
    "\n",
    "# Only look at inventors' first and second cities\n",
    "ip = ip.loc[(ip[\"inv_asg_rank\"] <= 1)]\n",
    "\n",
    "# Inventors' second cities\n",
    "sc = ip.loc[(ip[\"inv_asg_rank\"] == 1), [\"inventor_id\", \"inv_msa\", yv]].drop_duplicates([\"inventor_id\", \"inv_msa\"])\n",
    "\n",
    "# Inventors' second city compared to first\n",
    "ip[\"sec_inv_msa\"] = ip[\"inventor_id\"].map(dict(zip(sc[\"inventor_id\"], sc[\"inv_msa\"])))\n",
    "\n",
    "# Second city's first grant year\n",
    "ip[\"sec_fyear\"] = ip[\"inventor_id\"].map(dict(zip(sc[\"inventor_id\"], sc[yv])))\n",
    "\n",
    "# Get rid of the inventors whose second MSA matches the first\n",
    "ip = ip.loc[~(ip[\"inv_msa\"] == ip[\"sec_inv_msa\"])]\n",
    "print(len(ip))\n",
    "\n",
    "# Get rid inventors with any missing cities\n",
    "missing_cities = ip.loc[(ip[\"inv_msa\"].isnull() | ip[\"sec_inv_msa\"].isnull()), \"inventor_id\"].tolist()\n",
    "ip = ip.loc[~ip[\"inventor_id\"].isin(missing_cities)]\n",
    "print(len(ip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations to each of mobile inventors' patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 34.7 s, total: 2min 49s\n",
      "Wall time: 1min 25s\n",
      "CPU times: user 50 ms, sys: 0 ns, total: 50 ms\n",
      "Wall time: 47.2 ms\n",
      "CPU times: user 1.72 s, sys: 0 ns, total: 1.72 s\n",
      "Wall time: 1.62 s\n",
      "331597\n"
     ]
    }
   ],
   "source": [
    "cit = dd.read_parquet(\"RawData/Cleaned/cit_0628.parq\")\n",
    "\n",
    "%time c2 = cit[cit[\"cited\"].isin(ip[\"patent\"])].compute()\n",
    "\n",
    "# Remove self-citations\n",
    "asgs = pickle.load(open(\"RawData/Cleaned/patent_assignee_dict_0628.pkl\", \"rb\"))\n",
    "\n",
    "%time asg_match = (set(asgs.get(cited, [])).intersection(asgs.get(citing, [])) for cited, citing \\\n",
    "                   in zip(c2[\"cited\"], c2[\"citing\"]))\n",
    "%time asg_match = [len(i) for i in asg_match]\n",
    "c2[\"asg_match\"] = asg_match\n",
    "c2 = c2.loc[c2[\"asg_match\"] == 0]\n",
    "c2 = c2[[\"citing\", \"cited\"]]\n",
    "\n",
    "# Add assignees\n",
    "c2[\"cited_asg\"] = c2[\"cited\"].map(asgs)\n",
    "c2[\"citing_asg\"] = c2[\"citing\"].map(asgs)\n",
    "del(asgs)\n",
    "\n",
    "print(len(c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarities for citation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting row values\n",
      "2018-09-12 17:18:15.702909\n",
      "('ldavecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-12 17:18:18.019595\n",
      "Getting chunks\n",
      "2018-09-12 17:18:27.765365\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-12 17:18:28.850094\n",
      "finished\n",
      "2018-09-12 17:18:48.055166\n",
      "('docvecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-12 17:18:48.055292\n",
      "Getting chunks\n",
      "2018-09-12 17:19:06.406469\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-12 17:19:08.236596\n",
      "finished\n",
      "2018-09-12 17:19:34.887668\n"
     ]
    }
   ],
   "source": [
    "def grouper(n, iterable):\n",
    "    \"\"\"\n",
    "    >>> list(grouper(3, 'ABCDEFG'))\n",
    "    [['A', 'B', 'C'], ['D', 'E', 'F'], ['G']]\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    return iter(lambda: list(itertools.islice(iterable, n)), [])\n",
    "\n",
    "\n",
    "import scipy.spatial.distance as distance\n",
    "dms = [\"ldavecs\", \"docvecs\"]\n",
    "\n",
    "print(\"Getting row values\")\n",
    "print(datetime.datetime.now())\n",
    "pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "\n",
    "l2 = pd.DataFrame({\"tp\": c2[\"cited\"], \"op\": c2[\"citing\"]})\n",
    "\n",
    "for dm in dms:\n",
    "    print((dm,\"started\"))\n",
    "    print(\"Loading matrix and dict\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Store copy as array\n",
    "    l3 = l2.loc[l2[\"tp\"].isin(pat_dict.keys()) & l2[\"op\"].isin(pat_dict.keys()), [\"tp\", \"op\"]].copy()\n",
    "\n",
    "    if dm == \"ldavecs\":\n",
    "        ncols = 60\n",
    "    else:\n",
    "        ncols = 100\n",
    "\n",
    "    pm = fastparquet.ParquetFile(\"DataStore/2018-07-P2/ML/{0}_pats_0712.parq\".format(dm))\\\n",
    ".to_pandas().values\n",
    "\n",
    "    # Convert to chunks\n",
    "    print(\"Getting chunks\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Split into chunks\n",
    "    n_rows = 3000\n",
    "    n_chunks = int(np.round(len(l3)/n_rows))\n",
    "    tp_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"tp\"].iteritems()]])\n",
    "    op_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"op\"].iteritems()]])\n",
    "    del(pm)\n",
    "    chunks = itertools.zip_longest(tp_chunks, op_chunks)\n",
    "\n",
    "    print(\"Getting patent pair cosine similarity\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Cosine\n",
    "\n",
    "    cos_dis = np.empty(len(l3))\n",
    "\n",
    "    for r, c in enumerate(chunks):\n",
    "        cos_dis[r*n_rows:r*n_rows+n_rows] = np.diag(distance.cdist(c[0],c[1], metric = \"cosine\"))\n",
    "\n",
    "    l3[\"sim_{0}\".format(dm)] = 1-cos_dis\n",
    "    \n",
    "    # Rename columns\n",
    "    l3 = l3.rename(columns={\"tp\": \"cited\", \"op\": \"citing\"})\n",
    "    c2 = c2.merge(l3, how = \"left\", on = [\"cited\", \"citing\"])\n",
    "    del(l3)\n",
    "    print(\"finished\")\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get location for each patent\n",
    "# pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    "# .to_pandas([\"patent\", \"inv_msa\", \"gyear\", \"appyear\"])\n",
    "\n",
    "# Get MSA of cited patent\n",
    "c2[\"cited_inv_msa\"] = c2[\"cited\"].map(dict(zip(pdf[\"patent\"], pdf[\"inv_msa\"])))\n",
    "c2[\"citing_inv_msa\"] = c2[\"citing\"].map(dict(zip(pdf[\"patent\"], pdf[\"inv_msa\"])))\n",
    "\n",
    "# Get gyear of cited patent\n",
    "c2[\"cited_\"+yv] = c2[\"cited\"].map(dict(zip(pdf[\"patent\"], pdf[yv])))\n",
    "c2[\"citing_\"+yv] = c2[\"citing\"].map(dict(zip(pdf[\"patent\"], pdf[yv])))\n",
    "del(pdf)\n",
    "\n",
    "# Get second cities for each patent\n",
    "c2[\"sec_inv_msa\"] = c2[\"cited\"].map(dict(zip(ip[\"patent\"], ip[\"sec_inv_msa\"])))\n",
    "\n",
    "# Get second cities first grant year for each patent\n",
    "c2[\"sec_fyear\"] = c2[\"cited\"].map(dict(zip(ip[\"patent\"], ip[\"sec_fyear\"])))\n",
    "\n",
    "# Matching citing patent MSA to second MSA\n",
    "c2[\"sec_inv_msa_match\"] = (c2[\"citing_inv_msa\"] == c2[\"sec_inv_msa\"])\n",
    "\n",
    "# Match rate to second MSA\n",
    "# Before move\n",
    "prior = c2.loc[(c2[\"citing_\"+yv] < c2[\"sec_fyear\"]), [\"cited\", \"sec_inv_msa_match\"]].groupby(\"cited\").mean()\n",
    "# After move\n",
    "post = c2.loc[(c2[\"citing_\"+yv] >= c2[\"sec_fyear\"]), [\"cited\", \"sec_inv_msa_match\"]].groupby(\"cited\").mean()\n",
    "\n",
    "# Average similarity before and after move\n",
    "for c in [\"sim_docvecs\", \"sim_ldavecs\"]:\n",
    "    # Average of second MSA citations similarity to cited patent, before and after move\n",
    "    c3 = c2.loc[(c2[\"citing_\"+yv] < c2[\"sec_fyear\"]) & (c2[\"sec_inv_msa_match\"] == True),\n",
    "                [\"cited\", c]].groupby(\"cited\").mean()\n",
    "    c4 = c2.loc[(c2[\"citing_\"+yv] >= c2[\"sec_fyear\"]) & (c2[\"sec_inv_msa_match\"] == True),\n",
    "                [\"cited\", c]].groupby(\"cited\").mean()\n",
    "    prior = pd.concat([prior, c3], axis=1)\n",
    "    post = pd.concat([post, c4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10826\n",
      "10826\n"
     ]
    }
   ],
   "source": [
    "# Get match rate to second MSA for each patent\n",
    "for c in [\"sec_inv_msa_match\", \"sim_docvecs\", \"sim_ldavecs\"]:\n",
    "    ip[\"{0}_prior\".format(c)] = ip[\"patent\"].map(prior[c])\n",
    "    ip[\"{0}_post\".format(c)] = ip[\"patent\"].map(post[c])\n",
    "print(len(ip))\n",
    "# Get rid of the inventors whose second MSA matches the first\n",
    "ip = ip.loc[~(ip[\"inv_msa\"] == ip[\"sec_inv_msa\"])]\n",
    "print(len(ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_inv_msa_match_prior</th>\n",
       "      <th>sec_inv_msa_match_post</th>\n",
       "      <th>sim_docvecs_prior</th>\n",
       "      <th>sim_docvecs_post</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inv_asg_rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053176</td>\n",
       "      <td>0.088089</td>\n",
       "      <td>0.311498</td>\n",
       "      <td>0.311007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.083393</td>\n",
       "      <td>0.070523</td>\n",
       "      <td>0.317783</td>\n",
       "      <td>0.299359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sec_inv_msa_match_prior  sec_inv_msa_match_post  \\\n",
       "inv_asg_rank                                                    \n",
       "0                            0.053176                0.088089   \n",
       "1                            0.083393                0.070523   \n",
       "\n",
       "              sim_docvecs_prior  sim_docvecs_post  \n",
       "inv_asg_rank                                       \n",
       "0                      0.311498          0.311007  \n",
       "1                      0.317783          0.299359  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip[[\"inv_asg_rank\", \"sec_inv_msa_match_prior\", \"sec_inv_msa_match_post\",\n",
    "   \"sim_docvecs_prior\", \"sim_docvecs_post\"]].groupby(\"inv_asg_rank\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2.to_pickle(\"DataStore/2018-08/inv_move_cites_0912.pkl\")\n",
    "ip.to_pickle(\"DataStore/2018-08/inv_move_pats_0912.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare new assignees at new location's similarity to assignees who already cite prior patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yv = \"appyear\"\n",
    "c2 = pd.read_pickle(\"DataStore/2018-08/inv_move_cites_0912.pkl\")\n",
    "ip = pd.read_pickle(\"DataStore/2018-08/inv_move_pats_0912.pkl\")\n",
    "\n",
    "# Use unique assignees\n",
    "c2 = c2.drop([\"cited_asg\", \"citing_asg\"],1)\n",
    "asgs = fastparquet.ParquetFile(\"RawData/Cleaned/patent_assignees_unique_0628.parq\").to_pandas([\"patent\", \"assignee_id\"])\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\", \"primclass\", \"appyear\"])\n",
    "pdf = pdf.merge(asgs, how = \"left\", on = \"patent\")\n",
    "\n",
    "c2 = c2.merge(asgs, how=\"left\", left_on=\"citing\", right_on=\"patent\").rename(columns={\"assignee_id\": \"citing_asg\"}).drop(\"patent\",1)\n",
    "c2 = c2.merge(asgs, how=\"left\", left_on=\"cited\", right_on=\"patent\").rename(columns={\"assignee_id\": \"cited_asg\"}).drop(\"patent\",1)\n",
    "del(asgs)\n",
    "\n",
    "# New firms that cite prior patent post move\n",
    "a1 = c2.loc[(c2[\"citing_appyear\"] < c2[\"sec_fyear\"]), \"citing_asg\"].tolist()\n",
    "a2 = c2.loc[(c2[\"citing_appyear\"] >= c2[\"sec_fyear\"]), \"citing_asg\"].tolist()\n",
    "new_cite_asgs = list(set(a2).difference(set(a1)))\n",
    "prev_cite_asgs = list(set(a2).intersection(set(a1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csim = {}\n",
    "csim[\"prev_prior\"] = c2.loc[(c2[\"citing_\"+yv] < c2[\"sec_fyear\"]) & (c2[\"sec_inv_msa_match\"] == True)\\\n",
    "            & c2[\"citing_asg\"].isin(prev_cite_asgs),\n",
    "            [\"cited\", \"sim_docvecs\", \"sim_ldavecs\"]].groupby(\"cited\").mean().add_prefix(\"prev_prior_\").reset_index()\n",
    "csim[\"prev_post\"] = c2.loc[(c2[\"citing_\"+yv] >= c2[\"sec_fyear\"]) & (c2[\"sec_inv_msa_match\"] == True)\\\n",
    "            & c2[\"citing_asg\"].isin(prev_cite_asgs),\n",
    "            [\"cited\", \"sim_docvecs\", \"sim_ldavecs\"]].groupby(\"cited\").mean().add_prefix(\"prev_post_\").reset_index()\n",
    "csim[\"new_post\"] = c2.loc[(c2[\"citing_\"+yv] >= c2[\"sec_fyear\"]) & (c2[\"sec_inv_msa_match\"] == True)\\\n",
    "            & c2[\"citing_asg\"].isin(new_cite_asgs),\n",
    "            [\"cited\", \"sim_docvecs\", \"sim_ldavecs\"]].groupby(\"cited\").mean().add_prefix(\"new_post_\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get match rate to second MSA for each patent\n",
    "for k,v in csim.items():\n",
    "    ip = ip.merge(v, how=\"left\", left_on=\"patent\", right_on=\"cited\").drop(\"cited\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sec_inv_msa_match_prior    0.057886\n",
       "sec_inv_msa_match_post     0.084252\n",
       "sim_docvecs_prior          0.312521\n",
       "sim_docvecs_post           0.308676\n",
       "sim_ldavecs_prior          0.550057\n",
       "sim_ldavecs_post           0.527295\n",
       "prev_prior_sim_docvecs     0.312827\n",
       "prev_prior_sim_ldavecs     0.556394\n",
       "prev_post_sim_docvecs      0.291704\n",
       "prev_post_sim_ldavecs      0.526657\n",
       "new_post_sim_docvecs       0.316823\n",
       "new_post_sim_ldavecs       0.524154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip[['sec_inv_msa_match_prior',\n",
    "       'sec_inv_msa_match_post', 'sim_docvecs_prior', 'sim_docvecs_post',\n",
    "       'sim_ldavecs_prior', 'sim_ldavecs_post', 'prev_prior_sim_docvecs',\n",
    "       'prev_prior_sim_ldavecs', 'prev_post_sim_docvecs', 'prev_post_sim_ldavecs',\n",
    "       'new_post_sim_docvecs', 'new_post_sim_ldavecs']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
