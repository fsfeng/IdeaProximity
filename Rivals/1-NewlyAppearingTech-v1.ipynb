{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import datetime\n",
    "import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "import sklearn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "import fastparquet\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses *tp, op* where *op* is granted up to 10 years after *tp*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    "# .to_pandas()\n",
    "# dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# # Get relevant US Patents\n",
    "# pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats)]\n",
    "# # Add MSA subfield\n",
    "# pdf[\"msa_pc\"] = list(zip(pdf[\"inv_msa\"], pdf[\"primclass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Get number of primary class patents by MSA, and year of their first appearance\n",
    "p2 = pdf.groupby([\"inv_msa\", \"primclass\"]).size()\n",
    "# Only get primary classes with more than 50 patents\n",
    "p2 = p2.loc[(p2 >= 50)].reset_index().rename(columns={0:\"size\"})\n",
    "# Date of first appearance\n",
    "p3 = pdf[[\"inv_msa\", \"primclass\", \"gyear\"]].sort_values([\"inv_msa\", \"primclass\", \"gyear\"]).drop_duplicates([\"inv_msa\", \"primclass\"])\n",
    "# Merge\n",
    "p2 = p2.merge(p3, how=\"left\", on=[\"inv_msa\", \"primclass\"]).rename(columns={\"gyear\": \"first_gyear\"})\n",
    "\n",
    "# New technology fields after 1990\n",
    "p3 = p2.loc[(p2[\"first_gyear\"] >= 1990)]\n",
    "\n",
    "# Subfield\n",
    "p3[\"msa_pc\"] = list(zip(p3[\"inv_msa\"], p3[\"primclass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# # tp: patent from new subfield\n",
    "# # Primary class random sample\n",
    "# ns = fastparquet.ParquetFile(\"DataStore/2018-07-P2/Reg0726/reg_naics_name_sim_tr_0726.parq\").to_pandas()\n",
    "# ns[\"op_msa_pc\"] = list(zip(ns[\"op_inv_msa\"], ns[\"op_primclass\"]))\n",
    "# ns[\"tp_msa_pc\"] = list(zip(ns[\"tp_inv_msa\"], ns[\"tp_primclass\"]))\n",
    "# ns = ns.loc[ns[\"tp_msa_pc\"].isin(p3[\"msa_pc\"])]\n",
    "\n",
    "# Find highly similar PC\n",
    "ns2 = ns.loc[(ns[\"norm_sim_mean_docvecs_pc_msa\"] >= 1) &\n",
    "             (ns[\"inv_msa_match\"] == False), [\"tp_msa_pc\", \"op_msa_pc\",\n",
    "\"tp_primclass\", \"op_primclass\", \"tp_inv_msa\", \"op_inv_msa\", \"inv_msa_match\", \"year_group\", \n",
    "\"sim_mean_docvecs_pc_msa\", \"norm_sim_mean_docvecs_pc_msa\"]].drop_duplicates()\n",
    "\n",
    "# Comparison groups\n",
    "ns3 = ns2[[\"tp_msa_pc\", \"op_msa_pc\"]]\n",
    "ns3[\"tp_first_gyear\"] = ns3[\"tp_msa_pc\"].map(dict(zip(p3[\"msa_pc\"], p3[\"first_gyear\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.62 s, sys: 0 ns, total: 5.62 s\n",
      "Wall time: 5.66 s\n",
      "CPU times: user 15.6 s, sys: 83.2 ms, total: 15.7 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use only patents from relevant MSA-PC\n",
    "p2 = pdf.loc[pdf[\"msa_pc\"].isin(ns3[\"tp_msa_pc\"]) | pdf[\"msa_pc\"].isin(ns3[\"op_msa_pc\"])]\n",
    "\n",
    "# Patents by year\n",
    "%time pats_by_yr = {n:g[\"patent\"].tolist() for n,g in p2[[\"msa_pc\", \"gyear\", \"patent\"]].groupby([\"msa_pc\", \"gyear\"])}\n",
    "\n",
    "# Patents 10 years after\n",
    "pats_by_10_yr = {}\n",
    "for yr in range(1976,2010):\n",
    "    p = p2.loc[p2[\"gyear\"].isin(range(yr,yr+10))]\n",
    "    d = {(n,yr): g[\"patent\"].tolist() for n,g in p[[\"msa_pc\", \"patent\"]].groupby([\"msa_pc\"])}\n",
    "    pats_by_10_yr.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a grant year for each of the 10 years following the first grant year of new subfield\n",
    "ns4 = [list(zip([r1]*10, [r2]*10, [r3]*10, list(range(r3, r3+10)))) for r1,r2,r3\\\n",
    "       in zip(ns3[\"tp_msa_pc\"], ns3[\"op_msa_pc\"], ns3[\"tp_first_gyear\"])]\n",
    "ns4 = [item for sublist in ns4 for item in sublist]\n",
    "ns4 = pd.DataFrame({\"tp_msa_pc\": [i[0] for i in ns4],\n",
    "                   \"op_msa_pc\": [i[1] for i in ns4],\n",
    "                   \"tp_first_gyear\": [i[2] for i in ns4],\n",
    "                   \"gyear\": [i[3] for i in ns4],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26250\n",
      "19692\n"
     ]
    }
   ],
   "source": [
    "# Patents for each MSA-PC\n",
    "ns4[\"tp_msa_pc_pats\"] = [pats_by_yr.get((mp, yr), np.nan) for mp, yr in zip(ns4[\"tp_msa_pc\"], ns4[\"gyear\"])]\n",
    "ns4[\"tp_msa_pc_pats_10\"] = [pats_by_10_yr.get((mp, yr), np.nan) for mp, yr in zip(ns4[\"tp_msa_pc\"], ns4[\"gyear\"])]\n",
    "ns4[\"op_msa_pc_pats\"] = [pats_by_10_yr.get((mp, yr), np.nan) for mp, yr in zip(ns4[\"op_msa_pc\"], ns4[\"gyear\"])]\n",
    "print(len(ns4))\n",
    "# Drop whereever any of the patent lists are missing\n",
    "ns4 = ns4.dropna(subset=['tp_msa_pc_pats', 'op_msa_pc_pats',],how=\"any\")\n",
    "print(len(ns4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gyear</th>\n",
       "      <th>op_msa_pc</th>\n",
       "      <th>tp_first_gyear</th>\n",
       "      <th>tp_msa_pc</th>\n",
       "      <th>tp_msa_pc_pats</th>\n",
       "      <th>op_msa_pc_pats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991</td>\n",
       "      <td>(San Jose-Sunnyvale-Santa Clara, CA, 370.0)</td>\n",
       "      <td>1991</td>\n",
       "      <td>(Kansas City, MO-KS, 370.0)</td>\n",
       "      <td>[4999831]</td>\n",
       "      <td>[5007052, 5010544, 5012467, 5020058, 5043981, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>(San Jose-Sunnyvale-Santa Clara, CA, 370.0)</td>\n",
       "      <td>1991</td>\n",
       "      <td>(Kansas City, MO-KS, 370.0)</td>\n",
       "      <td>[5535200]</td>\n",
       "      <td>[5487066, 5488608, 5491685, 5493562, 5495485, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997</td>\n",
       "      <td>(San Jose-Sunnyvale-Santa Clara, CA, 370.0)</td>\n",
       "      <td>1991</td>\n",
       "      <td>(Kansas City, MO-KS, 370.0)</td>\n",
       "      <td>[5606553, 5636210]</td>\n",
       "      <td>[5592475, 5592486, 5594730, 5594734, 5596574, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1998</td>\n",
       "      <td>(San Jose-Sunnyvale-Santa Clara, CA, 370.0)</td>\n",
       "      <td>1991</td>\n",
       "      <td>(Kansas City, MO-KS, 370.0)</td>\n",
       "      <td>[5742605]</td>\n",
       "      <td>[5706274, 5710756, 5717688, 5719862, 5721725, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999</td>\n",
       "      <td>(San Jose-Sunnyvale-Santa Clara, CA, 370.0)</td>\n",
       "      <td>1991</td>\n",
       "      <td>(Kansas City, MO-KS, 370.0)</td>\n",
       "      <td>[5940393]</td>\n",
       "      <td>[5856975, 5859837, 5859852, 5864545, 5867501, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gyear                                    op_msa_pc  tp_first_gyear  \\\n",
       "0   1991  (San Jose-Sunnyvale-Santa Clara, CA, 370.0)            1991   \n",
       "5   1996  (San Jose-Sunnyvale-Santa Clara, CA, 370.0)            1991   \n",
       "6   1997  (San Jose-Sunnyvale-Santa Clara, CA, 370.0)            1991   \n",
       "7   1998  (San Jose-Sunnyvale-Santa Clara, CA, 370.0)            1991   \n",
       "8   1999  (San Jose-Sunnyvale-Santa Clara, CA, 370.0)            1991   \n",
       "\n",
       "                     tp_msa_pc      tp_msa_pc_pats  \\\n",
       "0  (Kansas City, MO-KS, 370.0)           [4999831]   \n",
       "5  (Kansas City, MO-KS, 370.0)           [5535200]   \n",
       "6  (Kansas City, MO-KS, 370.0)  [5606553, 5636210]   \n",
       "7  (Kansas City, MO-KS, 370.0)           [5742605]   \n",
       "8  (Kansas City, MO-KS, 370.0)           [5940393]   \n",
       "\n",
       "                                      op_msa_pc_pats  \n",
       "0  [5007052, 5010544, 5012467, 5020058, 5043981, ...  \n",
       "5  [5487066, 5488608, 5491685, 5493562, 5495485, ...  \n",
       "6  [5592475, 5592486, 5594730, 5594734, 5596574, ...  \n",
       "7  [5706274, 5710756, 5717688, 5719862, 5721725, ...  \n",
       "8  [5856975, 5859837, 5859852, 5864545, 5867501, ...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns4.to_pickle(\"DataStore/2018-08/related_field_pats_0918.pkl\")\n",
    "ns4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 3.78 s, total: 17.3 s\n",
      "Wall time: 17.4 s\n",
      "CPU times: user 25.5 s, sys: 2.09 s, total: 27.6 s\n",
      "Wall time: 28.4 s\n",
      "3109711\n"
     ]
    }
   ],
   "source": [
    "# Get sample pairs for each\n",
    "rs = {}\n",
    "%time msa_pc_pats = [list(itertools.product(i,j)) for i,j in zip(ns4[\"tp_msa_pc_pats\"], ns4[\"op_msa_pc_pats\"])]\n",
    "# Add identifiers\n",
    "%time msa_pc_pats = [list(zip(i, [(j,k,l,m)]*len(i))) for i,j,k,l,m in \\\n",
    "                    zip(msa_pc_pats, ns4[\"tp_msa_pc\"], ns4[\"op_msa_pc\"], ns4[\"gyear\"], ns4[\"tp_first_gyear\"])]\n",
    "# Sample half\n",
    "msa_pc_pats = (random.sample(l, int(np.round(len(l)/20))) for l in msa_pc_pats)\n",
    "msa_pc_pats = [item[0]+item[1] for sublist in msa_pc_pats for item in sublist]\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"tp\": [i[0] for i in msa_pc_pats],\n",
    "                           \"op\": [i[1] for i in msa_pc_pats],\n",
    "                            \"tp_msa_pc\": [i[2] for i in msa_pc_pats],\n",
    "                            \"op_msa_pc\": [i[3] for i in msa_pc_pats],\n",
    "                            \"gyear\": [i[4] for i in msa_pc_pats],\n",
    "                            \"tp_first_gyear\": [i[5] for i in msa_pc_pats],})\n",
    "d = d.drop_duplicates()\n",
    "d = d.loc[~(d[\"tp\"] == d[\"op\"])]\n",
    "\n",
    "# Drop same patent\n",
    "rs[\"msa_pc_pats\"] = d\n",
    "print(len(rs[\"msa_pc_pats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3109769\n"
     ]
    }
   ],
   "source": [
    "# msa_pc_pats = [list(itertools.product(i,j)) for i,j in zip(ns4[\"tp_msa_pc_pats\"], ns4[\"op_msa_pc_pats\"])]\n",
    "# # Add identifiers\n",
    "# msa_pc_pats = [list(zip(i, [(j,k,l,m)]*len(i))) for i,j,k,l,m in \n",
    "#                     zip(msa_pc_pats, ns4[\"tp_msa_pc\"], ns4[\"tp_msa_pc\"], ns4[\"gyear\"], ns4[\"tp_first_gyear\"])]\n",
    "# # Sample half\n",
    "# msa_pc_pats = (random.sample(l, int(np.round(len(l)/20))) for l in msa_pc_pats)\n",
    "# msa_pc_pats = [item[0]+item[1] for sublist in msa_pc_pats for item in sublist]\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"tp\": [i[0] for i in msa_pc_pats],\n",
    "                           \"op\": [i[1] for i in msa_pc_pats],\n",
    "                            \"tp_msa_pc\": [i[2] for i in msa_pc_pats],\n",
    "                            \"gyear\": [i[4] for i in msa_pc_pats],\n",
    "                            \"tp_first_gyear\": [i[5] for i in msa_pc_pats],})\n",
    "d = d.drop_duplicates()\n",
    "d = d.loc[~(d[\"tp\"] == d[\"op\"])]\n",
    "\n",
    "rs[\"tp_msa_pc_pats\"] = d\n",
    "\n",
    "print(len(rs[\"tp_msa_pc_pats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting row values\n",
      "2018-09-18 13:17:10.651913\n",
      "('ldavecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-18 13:17:12.628021\n",
      "3109711\n",
      "Getting chunks\n",
      "2018-09-18 13:17:38.208173\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 13:18:03.681198\n",
      "3109711\n",
      "finished\n",
      "2018-09-18 13:23:29.230606\n",
      "3109769\n",
      "Getting chunks\n",
      "2018-09-18 13:23:42.857297\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 13:24:04.289345\n",
      "3109769\n",
      "finished\n",
      "2018-09-18 13:30:54.543800\n",
      "('docvecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-18 13:30:54.544266\n",
      "3109711\n",
      "Getting chunks\n",
      "2018-09-18 13:31:29.542682\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 13:32:13.370671\n",
      "3109711\n",
      "finished\n",
      "2018-09-18 13:40:51.207710\n",
      "3109769\n",
      "Getting chunks\n",
      "2018-09-18 13:41:28.535275\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 13:42:23.108830\n",
      "3109769\n",
      "finished\n",
      "2018-09-18 13:50:54.788886\n"
     ]
    }
   ],
   "source": [
    "# Get similarity\n",
    "def grouper(n, iterable):\n",
    "    \"\"\"\n",
    "    >>> list(grouper(3, 'ABCDEFG'))\n",
    "    [['A', 'B', 'C'], ['D', 'E', 'F'], ['G']]\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    return iter(lambda: list(itertools.islice(iterable, n)), [])\n",
    "\n",
    "\n",
    "import scipy.spatial.distance as distance\n",
    "dms = [\"ldavecs\", \"docvecs\"]\n",
    "\n",
    "print(\"Getting row values\")\n",
    "print(datetime.datetime.now())\n",
    "pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "\n",
    "for dm in dms:\n",
    "    print((dm,\"started\"))\n",
    "    print(\"Loading matrix and dict\")\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    for k,v in rs.items():\n",
    "    \n",
    "        l2 = v.drop_duplicates()\n",
    "        print(len(l2))\n",
    "        # Store copy as array\n",
    "        l3 = l2.loc[l2[\"tp\"].isin(pat_dict.keys()) & l2[\"op\"].isin(pat_dict.keys()), [\"tp\", \"op\"]].copy().drop_duplicates()\n",
    "\n",
    "        if dm == \"ldavecs\":\n",
    "            ncols = 60\n",
    "        else:\n",
    "            ncols = 100\n",
    "\n",
    "        pm = fastparquet.ParquetFile(\"DataStore/2018-07-P2/ML/{0}_pats_0712.parq\".format(dm))\\\n",
    "    .to_pandas().values\n",
    "\n",
    "        # Convert to chunks\n",
    "        print(\"Getting chunks\")\n",
    "        print(datetime.datetime.now())\n",
    "        # Split into chunks\n",
    "        n_rows = 3000\n",
    "        n_chunks = int(np.round(len(l3)/n_rows))\n",
    "        tp_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"tp\"].iteritems()]])\n",
    "        op_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"op\"].iteritems()]])\n",
    "        del(pm)\n",
    "        chunks = itertools.zip_longest(tp_chunks, op_chunks)\n",
    "\n",
    "        print(\"Getting patent pair cosine similarity\")\n",
    "        print(datetime.datetime.now())\n",
    "        # Cosine\n",
    "\n",
    "        cos_dis = np.empty(len(l3))\n",
    "\n",
    "        for r, c in enumerate(chunks):\n",
    "            cos_dis[r*n_rows:r*n_rows+n_rows] = np.diag(distance.cdist(c[0],c[1], metric = \"cosine\"))\n",
    "\n",
    "        l3[\"{0}_{1}\".format(k, dm)] = 1-cos_dis\n",
    "\n",
    "        # Rename columns\n",
    "        l2 = l2.merge(l3.drop_duplicates(), how = \"left\", on = [\"tp\", \"op\"])\n",
    "        del(l3)\n",
    "        \n",
    "        # Update results\n",
    "        rs[k] = l2\n",
    "        print(len(l2))\n",
    "        print(\"finished\")\n",
    "        print(datetime.datetime.now())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(rs, open(\"DataStore/2018-08/related_pair_sim_0918.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = rs[\"msa_pc_pats\"][[\"tp_msa_pc\", \"op_msa_pc\", \"gyear\", \"msa_pc_pats_docvecs\"]]\\\n",
    ".groupby([\"tp_msa_pc\", \"op_msa_pc\", \"gyear\"]).mean().reset_index()\n",
    "r2 = rs[\"tp_msa_pc_pats\"][[\"tp_msa_pc\", \"gyear\", \"tp_msa_pc_pats_docvecs\"]]\\\n",
    ".groupby([\"tp_msa_pc\", \"gyear\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns4 = ns4.merge(r1, how=\"left\", on = [\"tp_msa_pc\", \"op_msa_pc\", \"gyear\"])\n",
    "ns4 = ns4.merge(r2, how=\"left\", on = [\"tp_msa_pc\", \"gyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns4.to_pickle(\"DataStore/2018-08/related_field_pats_0918.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gyear', 'op_msa_pc', 'tp_first_gyear', 'tp_msa_pc', 'tp_msa_pc_pats',\n",
       "       'op_msa_pc_pats', 'tp_msa_pc_pats_10', 'msa_pc_pats_docvecs',\n",
       "       'tp_msa_pc_pats_docvecs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
