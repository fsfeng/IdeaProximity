{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pprint\n",
    "import sklearn\n",
    "import gensim\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import fastparquet\n",
    "import string\n",
    "seed = 3\n",
    "\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading claims text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 2s, sys: 1min 5s, total: 22min 8s\n",
      "Wall time: 25min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iter_csv = pd.read_csv(\"RawData/PatentsView/claim.tsv\", sep='\\t', iterator=True, chunksize=100000)\n",
    "cl = pd.concat([chunk[chunk['sequence'] <= 3] for chunk in iter_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unduplicated US patents\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\"])\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# Get relevant US Patents\n",
    "pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats), \"patent\"].tolist()\n",
    "del(dup_pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17735902\n",
      "6551471\n",
      "CPU times: user 2min 9s, sys: 2.11 s, total: 2min 11s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "# # Number of texts\n",
    "# print(len(cl))\n",
    "\n",
    "# # Remove non utility patents\n",
    "# %time cl = cl.loc[cl[\"patent_id\"].apply(lambda x: x.isdigit() == True)]\n",
    "# cl[\"patent_id\"] = cl[\"patent_id\"].astype(int)\n",
    "# print(len(cl))\n",
    "\n",
    "# # Remove missing text\n",
    "# %time cl = cl.dropna(subset=[\"patent_id\", \"text\"])\n",
    "print(len(cl))\n",
    "\n",
    "# Remove patents not in relevant US patents\n",
    "cl = cl.loc[cl[\"patent_id\"].isin(pdf)]\n",
    "print(len(cl))\n",
    "\n",
    "# Reset index\n",
    "cl = cl.reset_index(drop=True)\n",
    "\n",
    "# Words to drop; have to set regex-True\n",
    "rem_dict = [\"1.\", \"2.\", \"3.\", \"4.\", \"5.\", \"claim 1\", \"claim 2\", \"claim 3\", \"claim 4\", \"claim 5\"]\n",
    "rem_dict = dict(zip(rem_dict, [\"\"]*len(rem_dict)))\n",
    "%time cl_text = cl[\"text\"].replace(rem_dict, regex=True)\n",
    "cl[\"text\"] = cl_text\n",
    "del(cl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4968079</td>\n",
       "      <td>A golf ball retriever having no moving parts c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6035330</td>\n",
       "      <td>A system for navigating a plurality of compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4031147</td>\n",
       "      <td>A process according to claim wherein said elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6188117</td>\n",
       "      <td>A gate electrode comprising:an insulative laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6883717</td>\n",
       "      <td>The secure credit card of claim wherein the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_id                                               text\n",
       "0    4968079  A golf ball retriever having no moving parts c...\n",
       "1    6035330  A system for navigating a plurality of compute...\n",
       "2    4031147  A process according to claim wherein said elec...\n",
       "3    6188117  A gate electrode comprising:an insulative laye...\n",
       "4    6883717   The secure credit card of claim wherein the c..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues saving claims text\n",
    "- Getting errors from fastparquet because there are too many rows? Solution using pyarrow here: https://stackoverflow.com/questions/50782252/pandas-to-parquet-fails-on-large-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = cl[[\"patent_id\", \"text\"]]\n",
    "cl.to_parquet(\"RawData/Cleaned/claims1112.parq\", engine=\"pyarrow\", compression=\"gzip\")\n",
    "# fastparquet.write(\"RawData/Cleaned/claims1112.parq\", cl, compression=\"GZIP\", row_group_offsets=20000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning claims text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.8 s, sys: 1.2 s, total: 60 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "# Load first\n",
    "%time cl = pd.concat((df_partial for df_partial in fastparquet.ParquetFile(\"RawData/Cleaned/claims1112.parq\").iter_row_groups()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 0 ns, total: 46 µs\n",
      "Wall time: 57.5 µs\n",
      "CPU times: user 10min 47s, sys: 3.52 s, total: 10min 50s\n",
      "Wall time: 10min 51s\n",
      "2220680\n"
     ]
    }
   ],
   "source": [
    "# Put all patents and claims together\n",
    "# %time cl = cl.groupby(\"patent_id\")\n",
    "%time claims = ((n, g[\"text\"].tolist()) for n,g in cl)\n",
    "%time claims = [(n, \" \".join(c)) for n, c in claims]\n",
    "\n",
    "cl2 = pd.DataFrame({\"patent\": [i[0] for i in claims], \"claims\": [i[1] for i in claims]})\n",
    "print(len(cl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patent to three merged claims texts\n",
    "# cl2.to_parquet(\"RawData/Cleaned/claims1112.parq\", engine=\"pyarrow\", compression=\"gzip\")\n",
    "del(cl, claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cl2.loc[0, \"claims\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import *\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, remove_stopwords, stem_text]\n",
    "%time cl_st = [\" \".join(preprocess_string(c, CUSTOM_FILTERS)) for c in iter(cl2[\"claims\"])]\n",
    "cl2[\"claims_stemmed\"] = cl_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2.to_parquet(\"RawData/Cleaned/claims1112.parq\", engine=\"pyarrow\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claims</th>\n",
       "      <th>patent</th>\n",
       "      <th>claims_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A bed arrangement comprisinga bed frame,a side...</td>\n",
       "      <td>3930273</td>\n",
       "      <td>bed arrang comprisinga bed frame rail assembl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An assembly as defined in claim whereinthe col...</td>\n",
       "      <td>3930274</td>\n",
       "      <td>assembl defin claim whereinth collaps contain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The method of claim wherein the heat-sealing o...</td>\n",
       "      <td>3930275</td>\n",
       "      <td>method claim heat seal step e carri moder die ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In a vehicle washing device, a passageway for ...</td>\n",
       "      <td>3930276</td>\n",
       "      <td>vehicl wash devic passagewai vehicl wheel have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The device of claim in which said plate means ...</td>\n",
       "      <td>3930277</td>\n",
       "      <td>devic claim said plate mean includ flat plate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              claims   patent  \\\n",
       "0  A bed arrangement comprisinga bed frame,a side...  3930273   \n",
       "1  An assembly as defined in claim whereinthe col...  3930274   \n",
       "2  The method of claim wherein the heat-sealing o...  3930275   \n",
       "3  In a vehicle washing device, a passageway for ...  3930276   \n",
       "4  The device of claim in which said plate means ...  3930277   \n",
       "\n",
       "                                      claims_stemmed  \n",
       "0  bed arrang comprisinga bed frame rail assembl ...  \n",
       "1  assembl defin claim whereinth collaps contain ...  \n",
       "2  method claim heat seal step e carri moder die ...  \n",
       "3  vehicl wash devic passagewai vehicl wheel have...  \n",
       "4  devic claim said plate mean includ flat plate ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing\n",
    "\n",
    "- Filtering min_df at least 0.0001 gave me only 9995 terms!\n",
    "- Filtering min_df at 0.00001 gave me 30889\n",
    "\n",
    "#### 1. First, find list of terms to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 s, sys: 589 ms, total: 55.4 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%time pats = pd.concat((df_partial for df_partial in fastparquet.ParquetFile(\"RawData/Cleaned/claims1112.parq\").iter_row_groups()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 15s, sys: 1.75 s, total: 5min 16s\n",
      "Wall time: 5min 14s\n",
      "30787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.1, min_df=0.00001, stop_words=\"english\")\n",
    "%time filt = cv.fit_transform(pats[\"claims_stemmed\"].tolist())\n",
    "\n",
    "# Get features\n",
    "filt = cv.get_feature_names()\n",
    "print(len(filt))\n",
    "del(cv)\n",
    "\n",
    "pd.Series(filt).to_pickle(\"RawData/Cleaned/claims_features_1120.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Filter each claims text\n",
    "Using generator gives huge speedup! Need to remember this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beverag contain dispens compris flexibl bag have sealabl open fill bag b valv attach wall bag said valv have oper member capabl movement close posit close valv close movement serv punctur bag said oper member movabl dispens posit open valv allow dispens beverag bag c support structur have oppos wall join diverg have flat join lower edg wall act base support structur bag upright displai dispens posit andd said valv project wall support structur beverag dispens construct compris structur claim combin rigid self support hous have open end said area support structur separ fit open end contain dispens claim wall support structur valv project area surround valv weaken readili remov remain attach valv'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pats.loc[10, \"claims_stemmed\"]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, filter_list):\n",
    "    for word in text.split():\n",
    "        if word in filter_list:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.6 ms ± 333 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.77 ms ± 38.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'beverag dispens flexibl bag sealabl bag valv wall bag valv capabl movement close close valv close movement serv punctur bag movabl dispens valv allow dispens beverag bag structur oppos wall join diverg flat join lower edg wall act structur bag upright displai dispens andd valv project wall structur beverag dispens construct structur combin rigid self hous area structur separ fit dispens wall structur valv project area surround valv weaken readili remov remain valv'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit \" \".join(tokenize(test, filt))\n",
    "print(\" \".join(tokenize(test, filt)))\n",
    "\n",
    "%timeit \" \".join(tokenize(test, set(filt)))\n",
    "print(\" \".join(tokenize(test, filt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:beverag dispens flexibl bag sealabl bag valv wall bag valv capabl movement close close valv close movement serv punctur bag movabl dispens valv allow dispens beverag bag structur oppos wall join diverg flat join lower edg wall act structur bag upright displai dispens andd valv project wall structur beverag dispens construct structur combin rigid self hous area structur separ fit dispens wall structur valv project area surround valv weaken readili remov remain valv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.6 ms ± 563 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:beverag dispens flexibl bag sealabl bag valv wall bag valv capabl movement close close valv close movement serv punctur bag movabl dispens valv allow dispens beverag bag structur oppos wall join diverg flat join lower edg wall act structur bag upright displai dispens andd valv project wall structur beverag dispens construct structur combin rigid self hous area structur separ fit dispens wall structur valv project area surround valv weaken readili remov remain valv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 ms ± 731 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "191 ms ± 11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:beverag dispens flexibl bag sealabl bag valv wall bag valv capabl movement close close valv close movement serv punctur bag movabl dispens valv allow dispens beverag bag structur oppos wall join diverg flat join lower edg wall act structur bag upright displai dispens andd valv project wall structur beverag dispens construct structur combin rigid self hous area structur separ fit dispens wall structur valv project area surround valv weaken readili remov remain valv\n"
     ]
    }
   ],
   "source": [
    "%timeit t = \" \".join([x for x in test.split(\" \") if x in filt])\n",
    "print(\" \".join([x for x in test.split(\" \") if x in filt]))\n",
    "%timeit t = \" \".join([x for x in test.split(\" \") if x in set(filt)])\n",
    "print(\" \".join([x for x in test.split(\" \") if x in set(filt)]))\n",
    "%timeit t = \" \".join((x for x in test.split(\" \") if x in set(filt)))\n",
    "print(\" \".join((x for x in test.split(\" \") if x in set(filt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the generator because the list comp scales speed O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.15 s ± 410 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "179 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "pl = pats[\"claims_stemmed\"][:100]\n",
    "%timeit t = [\" \".join([x for x in test.split(\" \") if x in filt]) for test in pl]\n",
    "# %timeit t = [\" \".join([x for x in test.split(\" \") if x in set(filt)]) for test in pl]\n",
    "# %timeit t = [\" \".join((x for x in test.split(\" \") if x in set(filt))) for test in pl]\n",
    "# %timeit t = [\" \".join(tokenize(test, filt)) for test in pl]\n",
    "%timeit t = [\" \".join(tokenize(test, set(filt))) for test in pl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:2018-11-21 13:12:59.177647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 1 ms, total: 144 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "# %time cl = (c.split() for c in iter(pats[\"claims_stemmed\"]))\n",
    "%time cl1 = (tokenize(c, set(filt)) for c in iter(pats[\"claims_stemmed\"]))\n",
    "%time cl = [\" \".join(c) for c in cl1]\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "print(\"Saving\")\n",
    "pats[\"claims_stemmed\"] = cl\n",
    "pats.to_parquet(\"RawData/Cleaned/claims1112.parq\", engine=\"pyarrow\", compression=\"gzip\")\n",
    "print(datetime.datetime.now())\n",
    "print(\"Finished saving\")\n",
    "del(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting DV on Claims\n",
    "\n",
    "1. First attent was just copying and pasting https://sfengc7.stern.nyu.edu:8888/notebooks/patentdiffusion/201808Results/FittingModels/1b-FitDV.ipynb, but produced terrible results\n",
    "2. Need to preprocess text to get rid of common and extremely uncommon terms as language is very common across all claims text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:good day to you madam fiona\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.addHandler(logging.FileHandler('Logs/DV_{0}.log'.format(datetime.datetime.now().\\\n",
    "                                                            strftime(\"%Y-%m-%d\"), 'a')))\n",
    "print = logging.info\n",
    "print('good day to you madam fiona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first\n",
    "print(\"Loading files\")\n",
    "%time pats = pd.concat((df_partial for df_partial in fastparquet.ParquetFile(\"RawData/Cleaned/claims1112.parq\").iter_row_groups()), axis=0)\n",
    "pats = pats.sample(frac=0.7, random_state = seed)\n",
    "claims_stemmed = pats[\"claims_stemmed\"].tolist()\n",
    "pat_labels = pats[\"patent\"].astype(str).tolist()\n",
    "del(pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            # print 'creating tagged document...%d' % idx\n",
    "            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n",
    "\n",
    "\n",
    "iterator = DocIterator(claims_stemmed, pat_labels)\n",
    "print('building vocabulary')\n",
    "\n",
    "%time model = gensim.models.Doc2Vec(size=100, window=10, min_count=50, workers=4, alpha=0.025, min_alpha=0.025)\n",
    "%time model.build_vocab(iterator)\n",
    "\n",
    "print('done building vocabulary')\n",
    "print('start training the model')\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "print(\"start\")\n",
    "print(starttime)\n",
    "model.train(iterator, total_examples=model.corpus_count, epochs=30)\n",
    "endtime = datetime.datetime.now()\n",
    "print(\"end\")\n",
    "print(endtime)\n",
    "\n",
    "model.save(\"DataStore/2018-07-P2/ML/doc2vec_claims_1119.model\")\n",
    "print(\"Finished saving model\")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model = gensim.models.doc2vec.Doc2Vec.load(\"DataStore/2018-07-P2/ML/doc2vec_claims_1119.model\")\n",
    "%time pa = pd.concat((df_partial for df_partial in fastparquet.ParquetFile(\"RawData/Cleaned/claims1112.parq\").iter_row_groups()), axis=0)\n",
    "%time pa_pats = pd.Series(list(range(len(pa))), index=pa[\"patent\"].tolist())\n",
    "%time pa_pats.to_pickle(\"RawData/Cleaned/pat_dict_claims_1120.pkl\")\n",
    "del(pa_pats)\n",
    "\n",
    "pa = pa[\"claims_stemmed\"].tolist()\n",
    "print(len(pa))\n",
    "try:\n",
    "    texts = [t.split() for t in pa]\n",
    "    print((len(texts), \"Text Length\"))\n",
    "    print(\"inferring new vectors\")\n",
    "    print(datetime.datetime.now())\n",
    "    new_vecs = [model.infer_vector(t) for t in texts]\n",
    "    print(\"finished inferring new vectors\")\n",
    "    print(datetime.datetime.now())\n",
    "    nv = pd.DataFrame(new_vecs, columns = [str(i) for i in range(100)], index = range(len(new_vecs)))\n",
    "    print((len(nv), \"New Vectors Length\"))\n",
    "    fastparquet.write(\"DataStore/2018-07-P2/ML/docvecs_claims_pats_1116.parq\", nv, compression = \"GZIP\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.exception(\"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220680"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2220680"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_pats = pd.read_pickle(\"RawData/Cleaned/pat_dict_claims_1120.pkl\")\n",
    "len(pa_pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
