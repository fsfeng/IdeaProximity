{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import timeit\n",
    "import pprint\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import gensim\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import fastparquet\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "\n",
    "seed = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:good day to you madam fiona\n",
      "INFO:root:started\n",
      "INFO:root:2018-07-13 13:23:12.141216\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.addHandler(logging.FileHandler('Logs/LDA_{0}.log'.format(datetime.datetime.now().\\\n",
    "                                                            strftime(\"%Y-%m-%d\"), 'a')))\n",
    "print = logging.info\n",
    "print('good day to you madam fiona')\n",
    "print('started')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on:\n",
    "- https://sfengc7.stern.nyu.edu:8888/edit/patentdiffusion/Scripts/LDAModel_Sampled_0605.py\n",
    "- https://sfengc7.stern.nyu.edu:8888/notebooks/patentdiffusion/201804KnowledgeSpilloversRep/2018-03-P1/0c-RefitMissingML.ipynb\n",
    "\n",
    "### First clean abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943842\n",
      "2943841\n",
      "2575725\n"
     ]
    }
   ],
   "source": [
    "# Merge existing with missing\n",
    "patabs = fastparquet.ParquetFile('PatBib/patabs7615_us.parq').to_pandas()\n",
    "pa = pd.read_pickle(\"PatBib/missing_abs_text_stemmed_0312.pkl\")\n",
    "patabs = patabs.append(pa, ignore_index = True)\n",
    "del(pa)\n",
    "print(len(patabs))\n",
    "\n",
    "# Convert to integer\n",
    "patabs[\"patent\"] = patabs[\"Patent\"].astype(int)\n",
    "patabs = patabs.drop(\"Patent\",1)\n",
    "\n",
    "# Record duplicates\n",
    "patabs = patabs.drop_duplicates(\"patent\")\n",
    "print(len(patabs))\n",
    "\n",
    "patdup = patabs.loc[patabs.duplicated(\"abs_stemmed\"), \"patent\"]\n",
    "patdup.to_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\")\n",
    "\n",
    "# Remove duplicates\n",
    "patabs = patabs.drop_duplicates(\"abs_stemmed\")\n",
    "print(len(patabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastparquet.write(\"RawData/Cleaned/patabs7615_us_no_dup.parq\", patabs, compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(patabs, patdup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Fit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_corpus(pattext_column):\n",
    "    texts = [t.split() for t in pattext_column.tolist()]\n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    # Resulting dictionary is around 41000, keep to 40000\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n = 40010)\n",
    "    dictionary.filter_n_most_frequent(10)\n",
    "    dictionary.compactify()\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def convert_corpus(pattext_column, samp_dict):\n",
    "    texts = [t.split() for t in pattext_column.tolist()]\n",
    "    corpus = [samp_dict.doc2bow(text) for text in texts]\n",
    "    return corpus\n",
    "    \n",
    "\n",
    "def convert_topic_vector(row):\n",
    "    tv = np.zeros(60)\n",
    "    for i in iter(pattopics[row]):\n",
    "        # Pattopics is (3, 0.05). So replace the i[0] = topic 3 with proportion i[1] = 0.05\n",
    "        tv[i[0]] = i[1]\n",
    "    return tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Fit LDA Model\n",
    "numtopics = 60\n",
    "\n",
    "# # Abstracts\n",
    "# patabs = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"abs_stemmed\"])\\\n",
    "# .sample(frac = 0.5, random_state = seed)[\"abs_stemmed\"]\n",
    "# print(len(patabs))                \n",
    "\n",
    "# # Generate and store dictionary & corpus\n",
    "# print(\"generate and store dictionary & corpus\")\n",
    "# starttime = datetime.datetime.now()\n",
    "# dict_samp, corpus_samp = gensim_corpus(patabs)\n",
    "\n",
    "# # Filter extremes\n",
    "# print(\"Dictionary: filter extremes\")\n",
    "# dict_samp.filter_extremes(no_below=20, no_above=0.1)\n",
    "\n",
    "# dict_samp.save('DataStore/2018-07-P2/ML/lda_dict_0712.dict')\n",
    "# gensim.corpora.MmCorpus.serialize('DataStore/2018-07-P2/ML/lda_corpus_0712.mm', corpus_samp)\n",
    "\n",
    "# endtime = datetime.datetime.now()\n",
    "# print(endtime)\n",
    "# print(\"dict & corpus creation ended\")\n",
    "#----------#\n",
    "# # Generate and store gensim LDA\n",
    "# print(\"fit lda\")\n",
    "# print(datetime.datetime.now())\n",
    "\n",
    "# print(\"Load dictionary & corpus\")\n",
    "# dict_samp = gensim.corpora.Dictionary.load('DataStore/2018-07-P2/ML/lda_dict_0712.dict')\n",
    "# corpus_samp = gensim.corpora.MmCorpus('DataStore/2018-07-P2/ML/lda_corpus_0712.mm')\n",
    "\n",
    "\n",
    "# print(datetime.datetime.now())\n",
    "# print(\"Fitting\")\n",
    "# lda = gensim.models.LdaModel(corpus_samp, id2word = dict_samp, num_topics = numtopics, random_state = seed,\n",
    "#                                 minimum_probability = 0.0)\n",
    "# lda.save(\"DataStore/2018-07-P2/ML/lda_samp_60_0712\")\n",
    "\n",
    "# # Delete the corpus generated by the sample\n",
    "# del(corpus_samp, dict_samp,lda)\n",
    "\n",
    "# print(\"Ended Fit\")\n",
    "# print(datetime.datetime.now())\n",
    "\n",
    "#---------#\n",
    "# Part 2: Convert topics to vectors\n",
    "\n",
    "# Create new corpus from sample dictionary\n",
    "numtopics = 60\n",
    "\n",
    "# Load abstract, dictionary, LDA model\n",
    "patabs = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"abs_stemmed\"])[\"abs_stemmed\"]\n",
    "dict_samp = gensim.corpora.Dictionary.load('DataStore/2018-07-P2/ML/lda_dict_0712.dict')\n",
    "lda = gensim.models.ldamodel.LdaModel.load('DataStore/2018-07-P2/ML/lda_samp_60_0712')\n",
    "\n",
    "# Convert full abstract corpus\n",
    "conv_corpus = convert_corpus(patabs, dict_samp)\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('DataStore/2018-07-P2/ML/lda_corpus_full_0712.mm', conv_corpus)\n",
    "\n",
    "# If it breaks\n",
    "# conv_corpus = gensim.corpora.MmCorpus('DataStore/2018-07-P2/ML/lda_corpus_full_0712.mm')\n",
    "\n",
    "# Save as topic\n",
    "pattopics = lda[conv_corpus]\n",
    "\n",
    "# Delete all files\n",
    "del(lda, conv_corpus, patabs, dict_samp)\n",
    "\n",
    "print(\"multiprocess topic to vec\")\n",
    "starttime = datetime.datetime.now()\n",
    "print(starttime)\n",
    "num_workers = 4    \n",
    "pool = multiprocessing.Pool(processes = multiprocessing.cpu_count()-1, maxtasksperchild=1000)\n",
    "\n",
    "out = []\n",
    "r = pool.map_async(convert_topic_vector, range(len(pattopics)), callback=out.append)  \n",
    "r.wait()\n",
    "\n",
    "out = out[0]\n",
    "\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime)\n",
    "print(\"topic to vec ended\")\n",
    "\n",
    "cols = [\"{0}\".format(i) for i in range(numtopics)]\n",
    "out = pd.DataFrame(out, columns = cols)\n",
    "\n",
    "fastparquet.write(\"DataStore/2018-07-P2/ML/tm_0712.parq\", out, compression = \"GZIP\")\n",
    "    \n",
    "# Part 3: Convert topic matrix to normalized topic matrix\n",
    "# This won't make the zero topics non-zero, only renormalies the \n",
    "print(\"renormalize topic mat\")\n",
    "starttime = datetime.datetime.now()\n",
    "print(starttime)\n",
    "# out = fastparquet.ParquetFile('DataStore/2017-06/tm_0603.parq').to_pandas()\n",
    "\n",
    "out2 = normalize(out, norm = \"l1\", axis = 1)\n",
    "out2 = pd.DataFrame(out2, columns = cols)\n",
    "\n",
    "# Save to pickle\n",
    "fastparquet.write(\"DataStore/2018-07-P2/ML/ldavecs_pats_0712.parq\", out2, compression = \"GZIP\")\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime)\n",
    "print(\"renormalization ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "\n",
    "## Loading matrices using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dv = dd.read_parquet(\"DataStore/2018-07-P2/ML/docvecs_pats_0712.parq\").values.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.63650155e-01, -3.88033455e-03, -8.33285302e-02, -3.37433219e-01,\n",
       "       -2.84327924e-01, -2.33318098e-02,  6.59138560e-02, -2.24007308e-01,\n",
       "       -1.01193190e-02,  9.62409079e-02, -5.00944592e-02, -5.05889617e-02,\n",
       "       -7.76260048e-02, -7.03641102e-02, -5.40510789e-02, -4.65533212e-02,\n",
       "       -7.73217753e-02, -1.35165066e-01,  2.59812474e-01, -5.28078787e-02,\n",
       "        1.26593217e-01, -1.40859634e-01, -3.55752409e-02,  2.19237253e-01,\n",
       "       -4.96096238e-02, -3.98100428e-02, -5.60732298e-02, -1.77158322e-02,\n",
       "       -1.07153758e-01,  7.30056986e-02,  1.73478767e-01, -1.95621755e-02,\n",
       "        3.78771387e-02, -1.12942904e-01,  2.69509722e-02,  1.16238251e-01,\n",
       "        1.26931027e-01, -5.77591099e-02, -1.00283951e-01, -4.21032906e-01,\n",
       "       -1.51542351e-01,  2.51701921e-01,  2.12049007e-01,  3.19784544e-02,\n",
       "       -1.65253907e-01,  8.97208452e-02,  2.35672757e-01,  1.84998810e-01,\n",
       "       -3.35111879e-02, -1.14441365e-01, -3.89985368e-02, -1.06185853e-01,\n",
       "        2.44154215e-01,  1.61595955e-01,  2.09104687e-01,  7.01227486e-02,\n",
       "        4.10909988e-02,  5.45771457e-02,  3.91711444e-01,  4.15329747e-02,\n",
       "        7.52442405e-02,  9.91880894e-02,  1.09571940e-03, -1.15176715e-01,\n",
       "        5.97169176e-02,  1.04592077e-01, -1.52264997e-01,  5.75410612e-02,\n",
       "        1.13675937e-01,  1.35687161e-02, -2.26357698e-01,  6.02710061e-05,\n",
       "       -7.82336518e-02, -3.66111070e-01, -4.37097102e-02, -1.47624388e-02,\n",
       "       -1.41688466e-01,  9.44149960e-03, -1.94598194e-02,  1.15140215e-01,\n",
       "       -3.00704557e-02,  4.51796800e-02, -6.01625368e-02, -9.90452915e-02,\n",
       "       -3.00298899e-01, -2.23588925e-02,  1.65145189e-01, -2.40119442e-01,\n",
       "        1.08829446e-01,  1.05166018e-01,  7.99310207e-02, -9.13762748e-02,\n",
       "        1.23112783e-01,  2.61778682e-02,  6.81124628e-02,  2.05181003e-01,\n",
       "       -2.50494964e-02,  1.88655332e-01, -2.74751544e-01,  1.37229860e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
