{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import datetime\n",
    "import time\n",
    "import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "import sklearn\n",
    "import dask\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "import fastparquet\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import collections\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create same of patents with common NPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 7.42 s, total: 38.5 s\n",
      "Wall time: 40.6 s\n",
      "8911717\n",
      "288369\n",
      "1826523\n",
      "CPU times: user 1min 43s, sys: 187 ms, total: 1min 43s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%time oc = fastparquet.ParquetFile(\"RawData/Cleaned/otherreference1016.parq\").to_pandas()\n",
    "\n",
    "# Only use available patents\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\"])\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# Get relevant US Patents\n",
    "pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats)]\n",
    "\n",
    "oc = oc.loc[oc[\"patent_id\"].isin(pdf[\"patent\"])]\n",
    "del(pdf)\n",
    "print(len(oc[\"ref_id\"].unique()))\n",
    "\n",
    "# Only select refernces with at least 3 patent references\n",
    "oc_m = oc[\"ref_id\"].value_counts()\n",
    "oc_m = oc_m[oc_m >= 3]\n",
    "# About 2.1 million: sample 33%\n",
    "oc_m = oc_m.sample(frac=0.33, random_state=3)\n",
    "print(len(oc_m))\n",
    "\n",
    "oc = oc.loc[oc[\"ref_id\"].isin(oc_m.index.tolist())]\n",
    "print(len(oc))\n",
    "\n",
    "# Convert to ref: patent list dictionary\n",
    "%time oc = {n:g[\"patent_id\"].tolist() for n,g in oc.groupby(\"ref_id\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 394 Âµs, total: 1.24 s\n",
      "Wall time: 1.25 s\n",
      "15037492\n",
      "10526244\n",
      "CPU times: user 26.6 s, sys: 954 ms, total: 27.5 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# All possible combinations of patent pairs sharing one common outside reference\n",
    "%time oc_l = [len(list(itertools.combinations(v, r=2))) for k,v in oc.items()]\n",
    "print(np.sum(oc_l))\n",
    "\n",
    "# Get all pairs and sample\n",
    "oc_l = (itertools.combinations(v, r=2) for k,v in oc.items())\n",
    "del(oc)\n",
    "# Get all pairs\n",
    "oc_l = [item for sublist in oc_l for item in sublist]\n",
    "\n",
    "# Sample 30%\n",
    "oc_l_ind = np.random.choice(np.shape(oc_l)[0], size=int(np.round(np.shape(oc_l)[0]*0.7)))\n",
    "oc_l = np.array(oc_l)[oc_l_ind]\n",
    "print(len(oc_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10526244\n",
      "2382230\n"
     ]
    }
   ],
   "source": [
    "oc_l = pd.DataFrame({\"tp\": [i[0] for i in oc_l],\n",
    "                    \"op\": [i[1] for i in oc_l]})\n",
    "print(len(oc_l))\n",
    "oc_l = oc_l.drop_duplicates()\n",
    "print(len(oc_l))\n",
    "fastparquet.write(\"DataStore/2018-10/mutual_npc_1027.parq\", oc_l, compression = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382230\n",
      "Loading matrix and dict\n",
      "2018-10-27 18:22:17.045083\n",
      "CPU times: user 28.7 s, sys: 3.39 s, total: 32.1 s\n",
      "Wall time: 11.2 s\n",
      "Getting row values\n",
      "2018-10-27 18:22:30.613511\n",
      "2382230\n",
      "1464315\n",
      "Getting chunks\n",
      "2018-10-27 18:22:34.394811\n",
      "Getting patent pair similarity\n",
      "cosine\n",
      "2018-10-27 18:22:51.781929\n",
      "2382230\n",
      "Finished ldavecs\n",
      "2018-10-27 18:26:32.724859\n",
      "Loading matrix and dict\n",
      "2018-10-27 18:26:32.724963\n",
      "CPU times: user 49.2 s, sys: 1.74 s, total: 51 s\n",
      "Wall time: 16.1 s\n",
      "Getting row values\n",
      "2018-10-27 18:26:50.693223\n",
      "2382230\n",
      "1464315\n",
      "Getting chunks\n",
      "2018-10-27 18:26:53.265869\n",
      "Getting patent pair similarity\n",
      "cosine\n",
      "2018-10-27 18:27:20.130966\n",
      "2382230\n",
      "Finished docvecs\n",
      "2018-10-27 18:32:29.346892\n",
      "2382230\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance as distance\n",
    "\n",
    "def grouper(n, iterable):\n",
    "    \"\"\"\n",
    "    >>> list(grouper(3, 'ABCDEFG'))\n",
    "    [['A', 'B', 'C'], ['D', 'E', 'F'], ['G']]\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    return iter(lambda: list(itertools.islice(iterable, n)), [])\n",
    "\n",
    "dms = [\"ldavecs\", \"docvecs\"]\n",
    "\n",
    "l2 = oc_l\n",
    "print(len(l2))\n",
    "for dm in dms:\n",
    "    print(\"Loading matrix and dict\")\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "    pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "    # Store as dask array\n",
    "    if dm == \"ldavecs\":\n",
    "        ncols = 60\n",
    "    else:\n",
    "        ncols = 100\n",
    "    dmf = \"DataStore/2018-07-P2/ML/{0}_pats_0712.parq\".format(dm)\n",
    "    %time pm = fastparquet.ParquetFile(dmf).to_pandas().values\n",
    "#     %time pm = da.from_array(pm, chunks=(10000,ncols))\n",
    "\n",
    "    print(\"Getting row values\")\n",
    "    print(datetime.datetime.now())\n",
    "    #----------#\n",
    "    # 2. Get pat vecs & pat similarity\n",
    "    print(len(l2))\n",
    "    # Remove missing values\n",
    "    l3 = l2[[\"tp\", \"op\"]].loc[l2[\"tp\"].isin(pat_dict.keys()) & l2[\"op\"].isin(pat_dict.keys())]\n",
    "\n",
    "    if dm == \"ldavecs\":\n",
    "        ncols = 60\n",
    "    else:\n",
    "        ncols = 100\n",
    "    print(len(l3))\n",
    "\n",
    "    print(\"Getting chunks\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Split into chunks\n",
    "    n_rows = 3000\n",
    "    n_chunks = int(np.round(len(l3)/n_rows))\n",
    "    tp_chunks = grouper(n_rows, pm[[pat_dict[p] for p in l3[\"tp\"].tolist()]])\n",
    "    op_chunks = grouper(n_rows, pm[[pat_dict[p] for p in l3[\"op\"].tolist()]])\n",
    "    chunks = itertools.zip_longest(tp_chunks, op_chunks)\n",
    "\n",
    "    print(\"Getting patent pair similarity\")\n",
    "    print(\"cosine\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Cosine\n",
    "\n",
    "    cos_dis = np.empty(len(l3))\n",
    "\n",
    "    for r, c in enumerate(chunks):\n",
    "        cos_dis[r*n_rows:r*n_rows+n_rows] = np.diag(distance.cdist(c[0],c[1], metric = \"cosine\"))\n",
    "\n",
    "    l3[\"sim_{0}\".format(dm)] = 1-cos_dis\n",
    "    del(cos_dis)\n",
    "\n",
    "    l2 = l2.merge(l3[[\"tp\", \"op\", \"sim_{0}\".format(dm)]], how = \"left\", on = [\"tp\", \"op\"])\n",
    "    print(len(l2))           \n",
    "    print(\"Finished {0}\".format(dm))\n",
    "    print(datetime.datetime.now())\n",
    "l2 = l2.drop_duplicates()\n",
    "print(len(l2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(oc_l)\n",
    "# l2 = l2.loc[l2[\"sim_docvecs\"].notnull()]\n",
    "# print(len(l2))\n",
    "fastparquet.write(\"DataStore/2018-10/mutual_npc_1027.parq\", l2, compression = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464315\n",
      "CPU times: user 7.38 s, sys: 0 ns, total: 7.38 s\n",
      "Wall time: 8.11 s\n",
      "1464315\n",
      "False    955553\n",
      "True     508762\n",
      "Name: common_pat_inv, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# inv = fastparquet.ParquetFile(\"RawData/Cleaned/patent_inventors_0628.parq\").to_pandas([\"patent\", \"inventor_id\"])\n",
    "# %time inv = {n: g[\"inventor_id\"].tolist() for n, g in inv.groupby(\"patent\")}\n",
    "\n",
    "mdc = l2\n",
    "print(len(mdc))\n",
    "%time num_common_inv = [len(set(inv[tp]).intersection(inv[op])) if (tp in inv.keys()) & (op in inv.keys())\\\n",
    "                  else np.nan for tp, op in zip(mdc[\"tp\"], mdc[\"op\"])]\n",
    "del(inv)\n",
    "mdc[\"num_common_pat_inv\"] = num_common_inv\n",
    "del(num_common_inv)\n",
    "mdc[\"common_pat_inv\"] = np.nan\n",
    "mdc.loc[mdc[\"num_common_pat_inv\"] >= 1, \"common_pat_inv\"] = True\n",
    "mdc.loc[mdc[\"num_common_pat_inv\"] == 0, \"common_pat_inv\"] = False\n",
    "mdc = mdc.drop(\"num_common_pat_inv\",1)\n",
    "print(len(mdc))\n",
    "print(mdc[\"common_pat_inv\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 46.1 s, total: 4min 35s\n",
      "Wall time: 1min 48s\n",
      "CPU times: user 1.03 s, sys: 673 ms, total: 1.7 s\n",
      "Wall time: 1.6 s\n",
      "CPU times: user 37.5 s, sys: 217 ms, total: 37.7 s\n",
      "Wall time: 37.5 s\n",
      "c2 length\n",
      "8673948\n",
      "CPU times: user 36.4 s, sys: 1.87 s, total: 38.3 s\n",
      "Wall time: 37 s\n",
      "CPU times: user 189 ms, sys: 234 ms, total: 423 ms\n",
      "Wall time: 108 ms\n",
      "CPU times: user 32.5 s, sys: 157 ms, total: 32.6 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "# asgs = pickle.load(open(\"RawData/Cleaned/patent_assignee_dict_0628.pkl\", \"rb\"))\n",
    "# cit = dd.read_parquet(\"RawData/Cleaned/cit_0628.parq\")\n",
    "\n",
    "# # Direct citations\n",
    "# # Create all False\n",
    "# mdc[\"direct_cite\"] = False\n",
    "# # Create zipped pairs\n",
    "# mdc[\"tp_op\"] = list(zip(mdc[\"tp\"], mdc[\"op\"]))\n",
    "# # Citations\n",
    "# c2 = cit[cit[\"cited\"].isin(mdc[\"tp\"])].compute()\n",
    "# mdc.loc[mdc[\"tp_op\"].isin(list(zip(c2[\"cited\"], c2[\"citing\"]))), \"direct_cite\"] = True\n",
    "# del(c2)\n",
    "# print(mdc[\"direct_cite\"].value_counts())\n",
    "# # Drop pairs\n",
    "# mdc = mdc.drop(\"tp_op\",1)\n",
    "\n",
    "# # Number of common citations\n",
    "# try:\n",
    "#     mdc = mdc.drop([\"num_common_cites\", \"common_cites_match\"],1)\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "%time c2 = cit[cit[\"citing\"].isin(mdc[\"tp\"]) | cit[\"citing\"].isin(mdc[\"op\"])].compute()\n",
    "\n",
    "# Remove self-citations\n",
    "%time asg_match = (set(asgs.get(cited, [])).intersection(asgs.get(citing, [])) for cited, citing \\\n",
    "                   in zip(c2[\"cited\"], c2[\"citing\"]))\n",
    "%time asg_match = [len(i) for i in asg_match]\n",
    "c2[\"asg_match\"] = asg_match\n",
    "c2 = c2.loc[c2[\"asg_match\"] == 0]\n",
    "c2 = c2[[\"citing\", \"cited\"]]\n",
    "print(\"c2 length\")\n",
    "print(len(c2))\n",
    "\n",
    "# Number of common citations after self-citation removal\n",
    "# Dictionary of citing: cited patents\n",
    "%time c2 = {n:g[\"cited\"].tolist() for n,g in c2.groupby(\"citing\")}\n",
    "\n",
    "# Total number of cites\n",
    "mdc[\"tp_num_cited\"] = [len(c2.get(p, [])) for p in mdc[\"tp\"]]\n",
    "mdc[\"op_num_cited\"] = [len(c2.get(p, [])) for p in mdc[\"op\"]]\n",
    "\n",
    "# Get number of overlapping\n",
    "%time num_common_cites = (set(c2.get(tp, [])).intersection(set(c2.get(op, []))) for tp, op in zip(mdc[\"tp\"], mdc[\"op\"]))\n",
    "%time mdc[\"num_common_cited\"] = [len(i) for i in num_common_cites]\n",
    "del(c2)\n",
    "\n",
    "# Pct of common cites\n",
    "mdc[\"tp_pct_common_cited\"] = mdc[\"num_common_cited\"]/mdc[\"tp_num_cited\"]\n",
    "# At least one number of common cites\n",
    "mdc[\"common_cited_match\"] = False\n",
    "mdc.loc[(mdc[\"num_common_cited\"] >= 1), \"common_cited_match\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastparquet.write(\"DataStore/2018-10/mutual_npc_1027.parq\", mdc, compression = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add other variables\n",
    "\n",
    "# Only use available patents\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\", \"gyear\", \"inv_msa\", \"naics_name\", \"primclass\"])\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# Get relevant US Patents\n",
    "pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats)]\n",
    "\n",
    "m2 = mdc.merge(pdf.add_prefix(\"tp_\"), how=\"left\", left_on=\"tp\", right_on=\"tp_patent\").drop(\"tp_patent\",1)\n",
    "m2 = m2.merge(pdf.add_prefix(\"op_\"), how=\"left\", left_on=\"op\", right_on=\"op_patent\").drop(\"op_patent\",1)\n",
    "\n",
    "m2[\"inv_msa_match\"] = (m2[\"tp_inv_msa\"] == m2[\"op_inv_msa\"])\n",
    "m2[\"primclass_match\"] = (m2[\"tp_primclass\"] == m2[\"op_primclass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdc = m2\n",
    "fastparquet.write(\"DataStore/2018-10/mutual_npc_1027.parq\", mdc, compression = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
