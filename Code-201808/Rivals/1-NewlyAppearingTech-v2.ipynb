{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import datetime\n",
    "import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "import sklearn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "import fastparquet\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing similarity to own MSA-PC patent field vs other similar patent fields\n",
    "- For new technology fields, analyse similarity to own MSA-PC patent field, vs other MSA-PCs\n",
    "    - Can be: (i) in same MSA, different PC; (ii) in different MSA, same PC; (iii) in different MSA, different PC\n",
    "    - There's a lot of traction for (ii) having pairs more similar\n",
    "    \n",
    "1. Identify primary classes that only reached over 20 patents per year after 1990, but with total patents greater than 1000\n",
    "2. Identify MSA-PC subfields with those primary classes and the first year they received more than 5 patents\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas()\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# Get relevant US Patents\n",
    "pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats)]\n",
    "# Add MSA subfield\n",
    "pdf[\"msa_pc\"] = list(zip(pdf[\"inv_msa\"], pdf[\"primclass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New technology fields\n",
    "pc_size = pdf[\"primclass\"].value_counts()\n",
    "pc_size = pc_size[pc_size > 1000]\n",
    "\n",
    "# Identify first year in which primary class exceeded 20 patents\n",
    "N = 20\n",
    "p2 = pdf.loc[pdf[\"primclass\"].isin(pc_size.index), [\"primclass\", \"gyear\", \"patent\"]]\\\n",
    ".groupby([\"primclass\", \"gyear\"]).size()\n",
    "p2 = p2.loc[(p2 >= N)].reset_index().rename(columns={0:\"size\"})\n",
    "\n",
    "# First year the field reaches more than N total patents\n",
    "p2 = p2[[\"primclass\", \"gyear\"]].drop_duplicates(subset=[\"primclass\"], keep=\"first\")\n",
    "pc_first = p2.loc[p2[\"gyear\"] >= 1985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Get size of msa_pc by year\n",
    "p2 = pdf[[\"inv_msa\", \"primclass\", \"gyear\", \"patent\"]].groupby([\"inv_msa\", \"primclass\", \"gyear\"]).size()\n",
    "p2 = p2.loc[(p2 >= 5)].reset_index().rename(columns={0:\"size\"})\n",
    "\n",
    "# Check number of years available\n",
    "p3 = p2.groupby([\"inv_msa\", \"primclass\"]).size()\n",
    "# Places with more than 10 years of 5 patents available\n",
    "p3 = p3.loc[(p3 >= 10)].reset_index().rename(columns={0:\"size\"}).drop(\"size\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2662\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "# First gyear\n",
    "p3 = p3.merge(p2[[\"inv_msa\", \"primclass\", \"gyear\"]].drop_duplicates(subset=[\"inv_msa\", \"primclass\"], keep=\"first\"),\n",
    "              how=\"left\", on=[\"inv_msa\",\"primclass\"]).rename(columns={\"gyear\": \"first_gyear\"})\n",
    "print(len(p3))\n",
    "\n",
    "# Only use new primary classes\n",
    "p3 = p3.loc[p3[\"primclass\"].isin(pc_first[\"primclass\"])]\n",
    "print(len(msa_pc_first))\n",
    "\n",
    "p3[\"msa_pc\"] = list(zip(p3[\"inv_msa\"], p3[\"primclass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# tp: patent from new subfield\n",
    "# Primary class random sample\n",
    "ns = fastparquet.ParquetFile(\"DataStore/2018-07-P2/Reg0726/reg_naics_name_sim_tr_0726.parq\").to_pandas()\n",
    "ns[\"op_msa_pc\"] = list(zip(ns[\"op_inv_msa\"], ns[\"op_primclass\"]))\n",
    "ns[\"tp_msa_pc\"] = list(zip(ns[\"tp_inv_msa\"], ns[\"tp_primclass\"]))\n",
    "ns = ns.loc[ns[\"tp_msa_pc\"].isin(p3[\"msa_pc\"])]\n",
    "\n",
    "# Find highly similar PC subfields not from the same MSA\n",
    "ns2 = ns.loc[(ns[\"norm_sim_mean_docvecs_pc_msa\"] >= 1.5) &\n",
    "             (ns[\"inv_msa_match\"] == False), [\"tp_msa_pc\", \"op_msa_pc\",\n",
    "\"tp_primclass\", \"op_primclass\", \"tp_inv_msa\", \"op_inv_msa\", \"inv_msa_match\", \"year_group\", \n",
    "\"sim_mean_docvecs_pc_msa\", \"norm_sim_mean_docvecs_pc_msa\"]].drop_duplicates()\n",
    "\n",
    "# Comparison groups\n",
    "ns3 = ns2[[\"tp_msa_pc\", \"op_msa_pc\"]]\n",
    "ns3[\"tp_first_gyear\"] = ns3[\"tp_msa_pc\"].map(dict(zip(p3[\"msa_pc\"], p3[\"first_gyear\"])))\n",
    "print(len(ns3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 570 µs, total: 1.58 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "# Use only patents from relevant MSA-PC\n",
    "p2 = pdf.loc[pdf[\"msa_pc\"].isin(ns3[\"tp_msa_pc\"]) | pdf[\"msa_pc\"].isin(ns3[\"op_msa_pc\"])]\n",
    "\n",
    "# Patents by year\n",
    "%time pats_by_yr = {n:g[\"patent\"].tolist() for n,g in p2[[\"msa_pc\", \"gyear\", \"patent\"]].groupby([\"msa_pc\", \"gyear\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21550\n",
      "43100\n"
     ]
    }
   ],
   "source": [
    "# Add a grant year for each of the 10 years following the first grant year of new subfield\n",
    "ns4 = [list(zip([r1]*10, [r2]*10, [r3]*10, list(range(r3, r3+10)))) for r1,r2,r3\\\n",
    "       in zip(ns3[\"tp_msa_pc\"], ns3[\"op_msa_pc\"], ns3[\"tp_first_gyear\"])]\n",
    "ns4 = [item for sublist in ns4 for item in sublist]\n",
    "print(len(ns4))\n",
    "# Same for self\n",
    "ns5 = [list(zip([r1]*10, [r2]*10, [r3]*10, list(range(r3, r3+10)))) for r1,r2,r3\\\n",
    "       in zip(ns3[\"tp_msa_pc\"], ns3[\"tp_msa_pc\"], ns3[\"tp_first_gyear\"])]\n",
    "ns5 = [item for sublist in ns5 for item in sublist]\n",
    "\n",
    "ns4.extend(ns5)\n",
    "print(len(ns4))\n",
    "\n",
    "ns4 = pd.DataFrame({\"tp_msa_pc\": [i[0] for i in ns4],\n",
    "                   \"op_msa_pc\": [i[1] for i in ns4],\n",
    "                   \"tp_first_gyear\": [i[2] for i in ns4],\n",
    "                   \"gyear\": [i[3] for i in ns4],})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43100\n",
      "41297\n"
     ]
    }
   ],
   "source": [
    "# Patents for each MSA-PC\n",
    "ns4[\"tp_msa_pc_pats\"] = [pats_by_yr.get((mp, yr), np.nan) for mp, yr in zip(ns4[\"tp_msa_pc\"], ns4[\"gyear\"])]\n",
    "ns4[\"op_msa_pc_pats\"] = [pats_by_yr.get((mp, yr), np.nan) for mp, yr in zip(ns4[\"op_msa_pc\"], ns4[\"gyear\"])]\n",
    "print(len(ns4))\n",
    "# Drop whereever any of the patent lists are missing\n",
    "ns4 = ns4.dropna(subset=['tp_msa_pc_pats', 'op_msa_pc_pats',],how=\"any\")\n",
    "print(len(ns4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns4.to_pickle(\"DataStore/2018-08/new_related_field_pats_0918.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.65 s, sys: 1.16 s, total: 6.81 s\n",
      "Wall time: 6.85 s\n",
      "CPU times: user 20 s, sys: 1.25 s, total: 21.3 s\n",
      "Wall time: 21.4 s\n",
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 10.7 µs\n",
      "CPU times: user 21.2 s, sys: 2.09 s, total: 23.3 s\n",
      "Wall time: 22.8 s\n",
      "2921115\n"
     ]
    }
   ],
   "source": [
    "# Get sample pairs for each\n",
    "%time msa_pc_pats = [list(itertools.product(i,j)) for i,j in zip(ns4[\"tp_msa_pc_pats\"], ns4[\"op_msa_pc_pats\"])]\n",
    "# Add identifiers\n",
    "%time msa_pc_pats = [list(zip(i, [(j,k,l,m)]*len(i))) for i,j,k,l,m in \\\n",
    "                    zip(msa_pc_pats, ns4[\"tp_msa_pc\"], ns4[\"op_msa_pc\"], ns4[\"gyear\"], ns4[\"tp_first_gyear\"])]\n",
    "# Sample half\n",
    "%time msa_pc_pats = (random.sample(l, int(np.round(len(l)/5))) for l in msa_pc_pats)\n",
    "%time msa_pc_pats = [item[0]+item[1] for sublist in msa_pc_pats for item in sublist]\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"tp\": [i[0] for i in msa_pc_pats],\n",
    "                           \"op\": [i[1] for i in msa_pc_pats],\n",
    "                            \"tp_msa_pc\": [i[2] for i in msa_pc_pats],\n",
    "                            \"op_msa_pc\": [i[3] for i in msa_pc_pats],\n",
    "                            \"gyear\": [i[4] for i in msa_pc_pats],\n",
    "                            \"tp_first_gyear\": [i[5] for i in msa_pc_pats],})\n",
    "d = d.drop_duplicates()\n",
    "d = d.loc[~(d[\"tp\"] == d[\"op\"])]\n",
    "\n",
    "# Drop same patent\n",
    "rs = d\n",
    "print(len(rs))\n",
    "del(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting row values\n",
      "2018-09-18 17:47:07.818841\n",
      "('ldavecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-18 17:47:10.791939\n",
      "2921115\n",
      "Getting chunks\n",
      "2018-09-18 17:47:22.129614\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 17:47:32.864066\n",
      "finished\n",
      "2018-09-18 17:52:41.302214\n",
      "('docvecs', 'started')\n",
      "Loading matrix and dict\n",
      "2018-09-18 17:52:41.302348\n",
      "2921115\n",
      "Getting chunks\n",
      "2018-09-18 17:53:00.849520\n",
      "Getting patent pair cosine similarity\n",
      "2018-09-18 17:53:20.013967\n",
      "finished\n",
      "2018-09-18 18:01:11.839995\n"
     ]
    }
   ],
   "source": [
    "# Get similarity\n",
    "def grouper(n, iterable):\n",
    "    \"\"\"\n",
    "    >>> list(grouper(3, 'ABCDEFG'))\n",
    "    [['A', 'B', 'C'], ['D', 'E', 'F'], ['G']]\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    return iter(lambda: list(itertools.islice(iterable, n)), [])\n",
    "\n",
    "\n",
    "import scipy.spatial.distance as distance\n",
    "dms = [\"ldavecs\", \"docvecs\"]\n",
    "\n",
    "print(\"Getting row values\")\n",
    "print(datetime.datetime.now())\n",
    "pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "\n",
    "l2 = rs.drop_duplicates()\n",
    "\n",
    "for dm in dms:\n",
    "    print((dm,\"started\"))\n",
    "    print(\"Loading matrix and dict\")\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    print(len(l2))\n",
    "    # Store copy as array\n",
    "    l3 = l2.loc[l2[\"tp\"].isin(pat_dict.keys()) & l2[\"op\"].isin(pat_dict.keys()), [\"tp\", \"op\"]].copy().drop_duplicates()\n",
    "\n",
    "    if dm == \"ldavecs\":\n",
    "        ncols = 60\n",
    "    else:\n",
    "        ncols = 100\n",
    "\n",
    "    pm = fastparquet.ParquetFile(\"DataStore/2018-07-P2/ML/{0}_pats_0712.parq\".format(dm))\\\n",
    ".to_pandas().values\n",
    "\n",
    "    # Convert to chunks\n",
    "    print(\"Getting chunks\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Split into chunks\n",
    "    n_rows = 3000\n",
    "    n_chunks = int(np.round(len(l3)/n_rows))\n",
    "    tp_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"tp\"].iteritems()]])\n",
    "    op_chunks = grouper(n_rows, pm[[pat_dict[p[1]] for p in l3[\"op\"].iteritems()]])\n",
    "    del(pm)\n",
    "    chunks = itertools.zip_longest(tp_chunks, op_chunks)\n",
    "\n",
    "    print(\"Getting patent pair cosine similarity\")\n",
    "    print(datetime.datetime.now())\n",
    "    # Cosine\n",
    "\n",
    "    cos_dis = np.empty(len(l3))\n",
    "\n",
    "    for r, c in enumerate(chunks):\n",
    "        cos_dis[r*n_rows:r*n_rows+n_rows] = np.diag(distance.cdist(c[0],c[1], metric = \"cosine\"))\n",
    "\n",
    "    l3[\"sim_{0}\".format(dm)] = 1-cos_dis\n",
    "    del(cos_dis)\n",
    "\n",
    "    # Rename columns\n",
    "    l2 = l2.merge(l3.drop_duplicates(), how = \"left\", on = [\"tp\", \"op\"])\n",
    "    del(l3)\n",
    "\n",
    "    print(\"finished\")\n",
    "    print(datetime.datetime.now())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gyear</th>\n",
       "      <th>op</th>\n",
       "      <th>op_msa_pc</th>\n",
       "      <th>tp</th>\n",
       "      <th>tp_first_gyear</th>\n",
       "      <th>tp_msa_pc</th>\n",
       "      <th>sim_docvecs</th>\n",
       "      <th>sim_ldavecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>6295526</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6317751</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "      <td>0.132798</td>\n",
       "      <td>0.394436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>6415281</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6408295</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "      <td>0.293622</td>\n",
       "      <td>0.233587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>6370523</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6341277</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "      <td>0.245861</td>\n",
       "      <td>0.058523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>6671703</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6662181</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "      <td>0.179977</td>\n",
       "      <td>0.263423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>6772151</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6785672</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "      <td>0.291113</td>\n",
       "      <td>0.160807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gyear       op                                    op_msa_pc       tp  \\\n",
       "0   2001  6295526  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6317751   \n",
       "1   2002  6415281  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6408295   \n",
       "2   2002  6370523  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6341277   \n",
       "3   2003  6671703  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6662181   \n",
       "4   2004  6772151  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6785672   \n",
       "\n",
       "   tp_first_gyear                                          tp_msa_pc  \\\n",
       "0            1996  (New York-Northern New Jersey-Long Island, NY-...   \n",
       "1            1996  (New York-Northern New Jersey-Long Island, NY-...   \n",
       "2            1996  (New York-Northern New Jersey-Long Island, NY-...   \n",
       "3            1996  (New York-Northern New Jersey-Long Island, NY-...   \n",
       "4            1996  (New York-Northern New Jersey-Long Island, NY-...   \n",
       "\n",
       "   sim_docvecs  sim_ldavecs  \n",
       "0     0.132798     0.394436  \n",
       "1     0.293622     0.233587  \n",
       "2     0.245861     0.058523  \n",
       "3     0.179977     0.263423  \n",
       "4     0.291113     0.160807  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gyear</th>\n",
       "      <th>op</th>\n",
       "      <th>op_msa_pc</th>\n",
       "      <th>tp</th>\n",
       "      <th>tp_first_gyear</th>\n",
       "      <th>tp_msa_pc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>6295526</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6317751</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>6415281</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6408295</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>6370523</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6341277</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>6671703</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6662181</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>6772151</td>\n",
       "      <td>(Atlanta-Sandy Springs-Marietta, GA, 707.0)</td>\n",
       "      <td>6785672</td>\n",
       "      <td>1996</td>\n",
       "      <td>(New York-Northern New Jersey-Long Island, NY-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gyear       op                                    op_msa_pc       tp  \\\n",
       "0   2001  6295526  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6317751   \n",
       "1   2002  6415281  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6408295   \n",
       "2   2002  6370523  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6341277   \n",
       "3   2003  6671703  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6662181   \n",
       "4   2004  6772151  (Atlanta-Sandy Springs-Marietta, GA, 707.0)  6785672   \n",
       "\n",
       "   tp_first_gyear                                          tp_msa_pc  \n",
       "0            1996  (New York-Northern New Jersey-Long Island, NY-...  \n",
       "1            1996  (New York-Northern New Jersey-Long Island, NY-...  \n",
       "2            1996  (New York-Northern New Jersey-Long Island, NY-...  \n",
       "3            1996  (New York-Northern New Jersey-Long Island, NY-...  \n",
       "4            1996  (New York-Northern New Jersey-Long Island, NY-...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = l2\n",
    "rs.to_pickle(\"DataStore/2018-08/new_field_pats_pairsim_0918.pkl\")\n",
    "del(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1 = rs[[\"tp_msa_pc\", \"op_msa_pc\", \"gyear\", \"sim_docvecs\"]]\\\n",
    ".groupby([\"tp_msa_pc\", \"op_msa_pc\", \"gyear\"]).mean().reset_index().rename(columns={\"sim_docvecs\": \"msa_pc_docvecs\"})\n",
    "ns4 = ns4.merge(r1, how=\"left\", on = [\"tp_msa_pc\", \"op_msa_pc\", \"gyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns4.to_pickle(\"DataStore/2018-08/new_related_field_pats_0918.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
