{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import datetime\n",
    "import time\n",
    "import pprint\n",
    "import itertools\n",
    "import pickle\n",
    "import sklearn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "import fastparquet\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Distances\n",
    "import scipy.spatial.distance as distance\n",
    "# KL\n",
    "from scipy.stats import entropy\n",
    "# Normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Pairwise distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from: https://sfengc7.stern.nyu.edu:8888/notebooks/patentdiffusion/Results/ExogSpillovers/2a-UniversityPatents-v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2523739\n",
      "2220706\n",
      "1918565\n",
      "18719\n",
      "369\n",
      "1756166\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\").to_pandas()\n",
    "print(len(pdf))\n",
    "pdf = pdf.drop_duplicates(\"patent\")\n",
    "# Remove actual duplicates\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\")\n",
    "pdf = pdf.loc[~(pdf[\"patent\"].isin(dup_pats))]\n",
    "print(len(pdf))\n",
    "\n",
    "def get_year_group(x):\n",
    "    if x in range(1975,1980):\n",
    "        yg = \"1975-80\"\n",
    "    elif x in range(1980,1985):\n",
    "        yg = \"1980-85\"\n",
    "    elif x in range(1985, 1990):\n",
    "        yg = \"1985-90\"\n",
    "    elif x in range(1990,1995):\n",
    "        yg = \"1990-95\"\n",
    "    elif x in range(1995,2000):\n",
    "        yg = \"1995-00\"\n",
    "    elif x in range(2000,2005):\n",
    "        yg = \"2000-05\"\n",
    "    elif x in range(2005,2010):\n",
    "        yg = \"2005-10\"\n",
    "    elif x in range(2010, 2015):\n",
    "        yg = \"2010-15\"\n",
    "    else:\n",
    "        yg = np.nan\n",
    "    return yg\n",
    "\n",
    "pdf[\"year_group\"] = pdf[\"gyear\"].apply(get_year_group)\n",
    "# Drop missing columns\n",
    "pdf = pdf.dropna(how=\"any\",subset=[\"gyear\", \"naics_name\", \"primclass\", \"year_group\"])\n",
    "print(len(pdf))\n",
    "\n",
    "# Drop locations with less than 200 patents\n",
    "vc = pdf[\"inv_msa\"].value_counts()\n",
    "print(len(vc))\n",
    "vc = vc[vc > 200].index.tolist()\n",
    "print(len(vc))\n",
    "pdf = pdf.loc[pdf[\"inv_msa\"].isin(vc)]\n",
    "print(len(pdf))\n",
    "\n",
    "# Only use patents granted 1976-2015\n",
    "pdf = pdf.loc[pdf[\"gyear\"].isin(range(1976,2015))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = pdf.sample(frac=0.3, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this sample: Sample less from own MSA and more from other MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(key, d, num):\n",
    "    try:\n",
    "        s = np.random.choice(d[key], size=num, replace=True)\n",
    "    except Exception:\n",
    "        s = [np.nan]*num\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naics_name\n",
      "2018-07-26 17:45:40.766773\n",
      "getting sample\n",
      "2018-07-26 17:46:03.251552\n",
      "2018-07-26 17:46:28.497517\n",
      "10537000\n",
      "10536820\n",
      "5230254\n",
      "cleaning assignees\n",
      "2018-07-26 17:46:34.063364\n",
      "CPU times: user 140 ms, sys: 104 ms, total: 244 ms\n",
      "Wall time: 242 ms\n",
      "CPU times: user 9.22 s, sys: 21 ms, total: 9.24 s\n",
      "Wall time: 9.16 s\n",
      "5030388\n",
      "4995523\n",
      "finished\n",
      "2018-07-26 17:47:57.941952\n",
      "primclass\n",
      "2018-07-26 17:47:57.942161\n",
      "getting sample\n",
      "2018-07-26 17:49:34.773994\n",
      "2018-07-26 17:49:48.163366\n",
      "10537000\n",
      "10532740\n",
      "5074573\n",
      "cleaning assignees\n",
      "2018-07-26 17:49:53.979071\n",
      "CPU times: user 153 ms, sys: 103 ms, total: 256 ms\n",
      "Wall time: 282 ms\n",
      "CPU times: user 8.02 s, sys: 10.9 ms, total: 8.04 s\n",
      "Wall time: 8.22 s\n",
      "4718622\n",
      "4580533\n",
      "finished\n",
      "2018-07-26 17:50:56.599692\n"
     ]
    }
   ],
   "source": [
    "k = \"year_group\"\n",
    "m = \"inv_msa\"\n",
    "\n",
    "for c in [\"naics_name\", \"primclass\"]:\n",
    "    print(c)\n",
    "    print(datetime.datetime.now())\n",
    "    # Relevant columns\n",
    "    cols = [c,k,m,\"gyear\", \"patent\"]\n",
    "    # Exclude fields with less than 100 patents\n",
    "    vc = pdf[c].value_counts()\n",
    "    vc = vc[vc > 100].index.tolist()\n",
    "    p_rel = pdf.loc[pdf[c].isin(vc), cols]\n",
    "    \n",
    "    num_in_msa = 5\n",
    "    num_out_msa = 15\n",
    "    # Each sample group\n",
    "    targ_g = {n: g[\"patent\"].tolist()*num_out_msa for n,g in targ[cols].groupby([c,k,m])} # Each group is from same field, MSA, year group\n",
    "    targ_g_m = {n: g[\"patent\"].tolist()*num_in_msa for n,g in targ[cols].groupby([c,k,m])}\n",
    "    p_c_m = {n: g[\"patent\"].tolist() for n,g in p_rel[cols].groupby([c,k,m])} # Each group is from same field, MSA, year group\n",
    "    p_c = {n: g[\"patent\"].tolist() for n,g in p_rel[cols].groupby([c,k])} # Each group from the same field, year group\n",
    "    ts = pd.DataFrame()\n",
    "    \n",
    "    print(\"getting sample\")\n",
    "    print(datetime.datetime.now())\n",
    "    # In MSA Sample\n",
    "    op_in_msa = (get_sample(n, p_c_m, len(g)) for n,g in targ_g_m.items())\n",
    "    op_in_msa = [item for sublist in op_in_msa for item in sublist]\n",
    "    # Field Sample: Group names NOT the same as target!\n",
    "    op_in_field = (get_sample((n[0],n[1]), p_c, len(g)) for n,g in targ_g.items())\n",
    "    op_in_field = [item for sublist in op_in_field for item in sublist]\n",
    "    # Target patents\n",
    "    tp = [item for sublist in targ_g.values() for item in sublist]\n",
    "    tp_m = [item for sublist in targ_g_m.values() for item in sublist]\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    s1 = pd.DataFrame({\"tp\": tp_m, \"op\": op_in_msa})\n",
    "    s1[\"samp\"] = \"In MSA\"\n",
    "    s2 = pd.DataFrame({\"tp\": tp, \"op\": op_in_field})\n",
    "    s2[\"samp\"] = \"In Field\"\n",
    "    \n",
    "    ts = s1.append(s2, ignore_index = True)\n",
    "    print(len(ts))\n",
    "    \n",
    "    # Drop missing\n",
    "    ts = ts.dropna(how=\"any\")\n",
    "    print(len(ts))\n",
    "    \n",
    "    # tp first\n",
    "    ts = ts.loc[ts[\"tp\"]<ts[\"op\"]]\n",
    "    print(len(ts))\n",
    "    \n",
    "    print(\"cleaning assignees\")\n",
    "    print(datetime.datetime.now())\n",
    "    asgs = pickle.load(open(\"RawData/Cleaned/patent_assignee_dict_0628.pkl\", \"rb\"))\n",
    "    # Check that target and other do not have same assignee\n",
    "    %time asg_match = (set(asgs.get(tp, [])).intersection(asgs.get(op, [])) for tp, op in zip(ts[\"tp\"], ts[\"op\"]))\n",
    "    %time asg_match = [len(i) for i in asg_match]\n",
    "\n",
    "    ts[\"asg_match\"] = asg_match\n",
    "    ts = ts.loc[ts[\"asg_match\"] == 0]\n",
    "    ts = ts.drop(\"asg_match\",1)\n",
    "    print(len(ts))\n",
    "    del(asgs)\n",
    "    \n",
    "    #Drop duplicates\n",
    "    ts = ts.drop_duplicates()\n",
    "    print(len(ts))\n",
    "    \n",
    "    # Add index otherwise it doesn't know how to sort\n",
    "    ts = ts.reset_index()\n",
    "    \n",
    "    ts[\"index2\"] = ts[\"index\"]\n",
    "    \n",
    "    # Convert to dask dataframe\n",
    "    ts = dd.from_pandas(ts, npartitions=1000)\n",
    "    # Save to parquet\n",
    "    dd.to_parquet(ts, path=\"DataStore/2018-07-P2/Reg0726/local_{0}_samp_0723.parq\".format(c), compression=\"gzip\")\n",
    "    \n",
    "    print(\"finished\")\n",
    "    print(datetime.datetime.now())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
