{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import fastparquet\n",
    "import os\n",
    "os.chdir('/mnt/t48/bighomes-active/sfeng/patentdiffusion/')\n",
    "seed = 3\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "import datetime\n",
    "import logging\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:good day to you madam fiona\n",
      "INFO:root:started\n",
      "INFO:root:2018-10-05 11:42:39.153621\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.addHandler(logging.FileHandler('Logs/pc_sim_{0}.log'.format(datetime.datetime.now().\\\n",
    "                                                            strftime(\"%Y-%m-%d\"), 'a')))\n",
    "print = logging.info\n",
    "print('good day to you madam fiona')\n",
    "print('started')\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load patents\n",
    "pdf = fastparquet.ParquetFile(\"RawData/Cleaned/patent_loc_unique_us_0628.parq\")\\\n",
    ".to_pandas([\"patent\", \"primclass\", \"inv_msa\", \"gyear\"])\n",
    "dup_pats = pd.read_pickle(\"RawData/Cleaned/duplicate_pattext_0712.pkl\").tolist()\n",
    "# Get relevant US Patents\n",
    "pdf = pdf.loc[~pdf[\"patent\"].isin(dup_pats)]\n",
    "# Get rid of missing observations\n",
    "pdf = pdf.dropna(how=\"any\")\n",
    "# Only use patents in patdict\n",
    "pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "# Get row values\n",
    "pdf[\"pat_row\"] = pdf[\"patent\"].map(pat_dict)\n",
    "pdf = pdf.loc[pdf[\"pat_row\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create patent sample dictionary for each year, PC, PC MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n",
      "2018-09-30 12:47:36.005610\n",
      "1981\n",
      "2018-09-30 12:47:46.764223\n",
      "1982\n",
      "2018-09-30 12:47:56.836563\n",
      "1983\n",
      "2018-09-30 12:48:16.434095\n",
      "1984\n",
      "2018-09-30 12:48:27.265533\n",
      "1985\n",
      "2018-09-30 12:48:38.474512\n",
      "1986\n",
      "2018-09-30 12:48:49.614143\n",
      "1987\n",
      "2018-09-30 12:49:00.140769\n",
      "1988\n",
      "2018-09-30 12:49:19.225333\n",
      "1989\n",
      "2018-09-30 12:49:30.598451\n",
      "1990\n",
      "2018-09-30 12:49:43.313347\n",
      "1991\n",
      "2018-09-30 12:49:55.128788\n",
      "1992\n",
      "2018-09-30 12:50:07.011631\n",
      "1993\n",
      "2018-09-30 12:50:20.170125\n",
      "1994\n",
      "2018-09-30 12:50:43.531859\n",
      "1995\n",
      "2018-09-30 12:50:56.786097\n",
      "1996\n",
      "2018-09-30 12:51:10.384832\n",
      "1997\n",
      "2018-09-30 12:51:25.016639\n",
      "1998\n",
      "2018-09-30 12:51:40.353776\n",
      "1999\n",
      "2018-09-30 12:52:03.907591\n",
      "2000\n",
      "2018-09-30 12:52:20.396601\n",
      "2001\n",
      "2018-09-30 12:52:36.581704\n",
      "2002\n",
      "2018-09-30 12:52:52.349374\n",
      "2003\n",
      "2018-09-30 12:53:08.518595\n",
      "2004\n",
      "2018-09-30 12:53:27.660108\n",
      "2005\n",
      "2018-09-30 12:53:45.881705\n",
      "2006\n",
      "2018-09-30 12:54:12.277115\n",
      "2007\n",
      "2018-09-30 12:54:30.268430\n",
      "2008\n",
      "2018-09-30 12:54:46.772478\n",
      "2009\n",
      "2018-09-30 12:55:00.168831\n",
      "2010\n",
      "2018-09-30 12:55:14.563335\n",
      "2011\n",
      "2018-09-30 12:55:27.743128\n",
      "2012\n",
      "2018-09-30 12:55:45.280517\n",
      "2013\n",
      "2018-09-30 12:55:59.970359\n",
      "2014\n",
      "2018-09-30 12:56:14.374779\n",
      "2015\n",
      "2018-09-30 12:56:37.430079\n"
     ]
    }
   ],
   "source": [
    "pc_dict = {}\n",
    "pc_msa_dict = {}\n",
    "max_size = 1000\n",
    "for yr in range(1980, 2016):\n",
    "    print(yr)\n",
    "    print(datetime.datetime.now())\n",
    "    # Control patent by assignee, primclass\n",
    "    p2 = pdf.loc[(pdf[\"gyear\"].isin(range(yr-5,yr)))]\n",
    "    \n",
    "    # Primary class patents\n",
    "    pcd = p2.groupby([\"primclass\"])\n",
    "    pcd = {(n,yr): g[\"pat_row\"].tolist() if (len(g) < max_size) else np.random.choice(g[\"pat_row\"].tolist(), \n",
    "            size=max_size, replace=False) for n,g in pcd}\n",
    "    pc_dict.update(pcd)\n",
    "    del(pcd)\n",
    "    \n",
    "    # PC MSA patents\n",
    "    pcmd = p2.groupby([\"primclass\", \"inv_msa\"])\n",
    "    pcmd = {n+(yr,): g[\"pat_row\"].tolist() if (len(g) < max_size) else np.random.choice(g[\"pat_row\"].tolist(), \n",
    "            size=max_size, replace=False) for n,g in pcmd}\n",
    "    pc_msa_dict.update(pcmd)\n",
    "    del(pcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(pc_dict, open(\"DataStore/2018-10/pc_year_pat_row_dict_0930.pkl\", \"wb\"))\n",
    "pickle.dump(pc_msa_dict, open(\"DataStore/2018-10/pc_msa_year_pat_row_dict_0930.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create list of PC, PC MSA pairs to get sample from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_dict = pickle.load(open(\"DataStore/2018-10/pc_year_pat_row_dict_0930.pkl\", \"rb\"))\n",
    "pc_msa_dict = pickle.load(open(\"DataStore/2018-10/pc_msa_year_pat_row_dict_0930.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:2868017\n",
      "INFO:root:2194513\n"
     ]
    }
   ],
   "source": [
    "files = {\"naics_name\": \"DataStore/2018-08/Reg0919/naics_name_all_0824.parq\",\n",
    "         \"primclass\": \"DataStore/2018-08/Reg0919/primclass_all_0824.parq\",\n",
    "}\n",
    "\n",
    "ps = pd.DataFrame()\n",
    "for k,v in files.items():\n",
    "    l = fastparquet.ParquetFile(v).to_pandas([\"tp_primclass\", \"tp_inv_msa\", \"tp_gyear\",\n",
    "                                              \"op_primclass\", \"op_inv_msa\"])\n",
    "    ps = ps.append(l, ignore_index=True)\n",
    "# Drop duplicates\n",
    "print(len(ps))\n",
    "ps = ps.drop_duplicates()\n",
    "print(len(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp_primclass</th>\n",
       "      <th>tp_inv_msa</th>\n",
       "      <th>tp_gyear</th>\n",
       "      <th>op_primclass</th>\n",
       "      <th>op_inv_msa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296.0</td>\n",
       "      <td>Clinton Township, MI</td>\n",
       "      <td>2000</td>\n",
       "      <td>296.0</td>\n",
       "      <td>Clinton Township, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.0</td>\n",
       "      <td>Buffalo-Niagara Falls, NY</td>\n",
       "      <td>2001</td>\n",
       "      <td>101.0</td>\n",
       "      <td>Buffalo-Niagara Falls, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347.0</td>\n",
       "      <td>San Diego-Carlsbad-San Marcos, CA</td>\n",
       "      <td>2001</td>\n",
       "      <td>726.0</td>\n",
       "      <td>San Francisco-Oakland-Fremont, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>370.0</td>\n",
       "      <td>New York-Northern New Jersey-Long Island, NY-N...</td>\n",
       "      <td>1996</td>\n",
       "      <td>372.0</td>\n",
       "      <td>New York-Northern New Jersey-Long Island, NY-N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137.0</td>\n",
       "      <td>Philadelphia-Camden-Wilmington, PA-NJ-DE-MD</td>\n",
       "      <td>1991</td>\n",
       "      <td>269.0</td>\n",
       "      <td>Portland-Vancouver-Hillsboro, OR-WA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tp_primclass                                         tp_inv_msa  tp_gyear  \\\n",
       "0         296.0                               Clinton Township, MI      2000   \n",
       "1          62.0                          Buffalo-Niagara Falls, NY      2001   \n",
       "2         347.0                  San Diego-Carlsbad-San Marcos, CA      2001   \n",
       "3         370.0  New York-Northern New Jersey-Long Island, NY-N...      1996   \n",
       "4         137.0        Philadelphia-Camden-Wilmington, PA-NJ-DE-MD      1991   \n",
       "\n",
       "   op_primclass                                         op_inv_msa  \n",
       "0         296.0                               Clinton Township, MI  \n",
       "1         101.0                          Buffalo-Niagara Falls, NY  \n",
       "2         726.0                  San Francisco-Oakland-Fremont, CA  \n",
       "3         372.0  New York-Northern New Jersey-Long Island, NY-N...  \n",
       "4         269.0                Portland-Vancouver-Hillsboro, OR-WA  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Similarity by PC pair, PC MSA pair\n",
    "\n",
    "#### 3.1 PC Pair\n",
    "\n",
    "Use only lower diagonal values of similarity matrix because the diagonals all equal 1, and only use similarities not equal to 0 (hopefully will be all the upper triangular zeroes, and few actual similarities) and less than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_dict = {k: np.array(v, dtype=int) for k,v in pc_dict.items() if len(v) > 0}\n",
    "pc_msa_dict = {k: np.array(v, dtype=int) for k,v in pc_msa_dict.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading pat dict\n",
      "INFO:root:2018-10-05 13:11:31.599093\n",
      "INFO:root:('ldavecs', 'started')\n",
      "INFO:root:Loading matrix and dict\n",
      "INFO:root:2018-10-05 13:11:33.714288\n",
      "INFO:root:Getting distance\n",
      "INFO:root:2018-10-05 13:11:54.511321\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/homedir/eco/sfeng/bigdata/python/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "INFO:root:2194513\n",
      "INFO:root:2194513\n",
      "INFO:root:Finished PC\n",
      "INFO:root:2018-10-05 13:30:59.906282\n",
      "INFO:root:Getting distance\n",
      "INFO:root:2018-10-05 13:31:11.809395\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance as distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dms = [\"ldavecs\", \"docvecs\"]\n",
    "\n",
    "print(\"Loading pat dict\")\n",
    "print(datetime.datetime.now())\n",
    "pat_dict = fastparquet.ParquetFile(\"RawData/Cleaned/patabs7615_us_no_dup.parq\").to_pandas([\"patent\"])[\"patent\"].tolist()\n",
    "pat_dict = dict(zip(pat_dict, range(len(pat_dict))))\n",
    "\n",
    "l1 = ps.copy()\n",
    "for dm in dms:\n",
    "    print((dm,\"started\"))\n",
    "    print(\"Loading matrix and dict\")\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    pm = fastparquet.ParquetFile(\"DataStore/2018-07-P2/ML/{0}_pats_0712.parq\".format(dm))\\\n",
    "    .to_pandas().values\n",
    "    \n",
    "    # 1. Similarity between primary classes\n",
    "    \n",
    "    # Drop duplicate primary class year pairs\n",
    "    l2 = ps[[\"tp_primclass\", \"op_primclass\", \"tp_gyear\"]].copy().drop_duplicates()\n",
    "    l2[\"tp_pc_ind\"] = list(zip(l2[\"tp_primclass\"], l2[\"tp_gyear\"]))\n",
    "    l2[\"op_pc_ind\"] = list(zip(l2[\"op_primclass\"], l2[\"tp_gyear\"]))\n",
    "    # Only get values in dictionary\n",
    "    l2 = l2.loc[l2[\"tp_pc_ind\"].isin(pc_dict.keys()) & l2[\"op_pc_ind\"].isin(pc_dict.keys())]\n",
    "    \n",
    "    print(\"Getting distance\")\n",
    "    print(datetime.datetime.now())\n",
    "    iter_mat = iter(((pm[pc_dict[i]], pm[pc_dict[j]]) for i,j in zip(l2[\"tp_pc_ind\"], l2[\"op_pc_ind\"])))\n",
    "    \n",
    "    dis = np.zeros(len(l2))\n",
    "    sd = np.zeros(len(l2))\n",
    "    for n,i in enumerate(iter_mat):\n",
    "        s = np.tril(cosine_similarity(i[0],i[1]),0)\n",
    "        s = s[(s != 0) & (s < 1)]\n",
    "        dis[n] = np.mean(s)\n",
    "        sd[n] = np.std(s)\n",
    "    \n",
    "    l2[\"mean_sim_{0}_pc\".format(dm)] = dis\n",
    "    l2[\"sd_sim_{0}_pc\".format(dm)] = sd\n",
    "    del(dis, sd)\n",
    "    \n",
    "    # Normalize values\n",
    "    l2[\"norm_mean_sim_{0}_pc\".format(dm)] = np.nan\n",
    "    l2.loc[l2[\"mean_sim_{0}_pc\".format(dm)].notnull(), \"norm_mean_sim_{0}_pc\".format(dm)] =\\\n",
    "    scaler.fit_transform(l2.loc[l2[\"mean_sim_{0}_pc\".format(dm)].notnull(), \"mean_sim_{0}_pc\".format(dm)]\\\n",
    "                         .values.reshape(-1,1))\n",
    "    \n",
    "    # Merge with original data\n",
    "    print(len(l1))\n",
    "    l1 = l1.merge(l2, how=\"left\", on = [\"tp_primclass\", \"op_primclass\", \"tp_gyear\"])\n",
    "    l1.to_pickle(\"DataStore/2018-10/temp_sim_save.pkl\")\n",
    "    print(len(l1))\n",
    "    del(l2)\n",
    "    print(\"Finished PC\")\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    # 2. Similarity between PC-MSA\n",
    "    \n",
    "    # Drop duplicates\n",
    "    l2 = ps.copy().drop_duplicates()\n",
    "    l2[\"tp_pc_m_ind\"] = list(zip(l2[\"tp_primclass\"], l2[\"tp_inv_msa\"], l2[\"tp_gyear\"]))\n",
    "    l2[\"op_pc_m_ind\"] = list(zip(l2[\"op_primclass\"], l2[\"op_inv_msa\"], l2[\"tp_gyear\"]))\n",
    "    # Only get values in dictionary\n",
    "    l2 = l2.loc[l2[\"tp_pc_m_ind\"].isin(pc_msa_dict.keys()) & l2[\"op_pc_m_ind\"].isin(pc_msa_dict.keys())]\n",
    "    \n",
    "    print(\"Getting distance\")\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    iter_mat = iter(((pm[pc_msa_dict[i]], pm[pc_msa_dict[j]]) for i,j in zip(l2[\"tp_pc_m_ind\"], l2[\"op_pc_m_ind\"])))\n",
    "    \n",
    "    dis = np.zeros(len(l2))\n",
    "    sd = np.zeros(len(l2))\n",
    "    for n,i in enumerate(iter_mat):\n",
    "        s = np.tril(cosine_similarity(i[0],i[1]),0)\n",
    "        s = s[(s != 0) & (s < 1)]\n",
    "        dis[n] = np.mean(s)\n",
    "        sd[n] = np.std(s)\n",
    "        \n",
    "    l2[\"mean_sim_{0}_pc_msa\".format(dm)] = dis\n",
    "    l2[\"sd_sim_{0}_pc_msa\".format(dm)] = sd\n",
    "    del(dis, sd)\n",
    "    \n",
    "    # Normalize values\n",
    "    l2[\"norm_mean_sim_{0}_pc_msa\".format(dm)] = np.nan\n",
    "    l2.loc[l2[\"mean_sim_{0}_pc_msa\".format(dm)].notnull(), \"norm_mean_sim_{0}_pc_msa\".format(dm)] =\\\n",
    "    scaler.fit_transform(l2.loc[l2[\"mean_sim_{0}_pc_msa\".format(dm)].notnull(), \"mean_sim_{0}_pc_msa\".format(dm)]\\\n",
    "                         .values.reshape(-1,1))\n",
    "    \n",
    "    print(len(l1))\n",
    "    l1 = l1.merge(l2, how=\"left\", on = [\"tp_primclass\", \"tp_inv_msa\", \"op_primclass\", \"op_inv_msa\", \"tp_gyear\"])\n",
    "    l1.to_pickle(\"DataStore/2018-10/temp_sim_save.pkl\")\n",
    "    print(len(l1))\n",
    "    del(l2)\n",
    "    \n",
    "    print(\"Finished PC MSA\")\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = l1[['tp_primclass', 'tp_inv_msa', 'tp_gyear', 'op_primclass', 'op_inv_msa',\n",
    "        \"mean_sim_docvecs_pc\", \"mean_sim_ldavecs_pc\", \"mean_sim_docvecs_pc_msa\", \"mean_sim_ldavecs_pc_msa\",\n",
    "         \"norm_mean_sim_docvecs_pc\", \"norm_mean_sim_ldavecs_pc\", \"norm_mean_sim_docvecs_pc_msa\",\n",
    "         \"norm_mean_sim_ldavecs_pc_msa\",\n",
    "        \"sd_sim_docvecs_pc\", \"sd_sim_ldavecs_pc\", \"sd_sim_docvecs_pc_msa\", \"sd_sim_ldavecs_pc_msa\"]]\n",
    "fastparquet.write(\"DataStore/2018-10/pc_sim_pairs_0929.parq\", ps, compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "Because I was stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2 = l1.copy()\n",
    "dup = l2.columns.get_duplicates()\n",
    "cols = pd.Series(l2.columns)\n",
    "for d in dup:\n",
    "    cols.loc[cols==d] = [d+\"_{0}\".format(i) for i in range(len(cols.loc[cols==d]))]\n",
    "l2.columns = list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2 = l2[['tp_primclass', 'tp_inv_msa', 'tp_gyear', 'op_primclass', 'op_inv_msa',\n",
    "\"mean_sim_docvecs_pc_x_1\", \"sd_sim_docvecs_pc_x_1\", \"mean_sim_ldavecs_pc_x_1\", \"sd_sim_ldavecs_pc_x_1\",\n",
    "\"mean_sim_docvecs_pc_msa\", \"sd_sim_docvecs_pc_msa\", \"mean_sim_ldavecs_pc_msa\", \"sd_sim_ldavecs_pc_msa\"]]\n",
    "l2.columns = ['tp_primclass', 'tp_inv_msa', 'tp_gyear', 'op_primclass', 'op_inv_msa',\n",
    "\"mean_sim_docvecs_pc\", \"sd_sim_docvecs_pc\", \"mean_sim_ldavecs_pc\", \"sd_sim_ldavecs_pc\",\n",
    "\"mean_sim_docvecs_pc_msa\", \"sd_sim_docvecs_pc_msa\", \"mean_sim_ldavecs_pc_msa\", \"sd_sim_ldavecs_pc_msa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = l2[['tp_primclass', 'tp_inv_msa', 'tp_gyear', 'op_primclass', 'op_inv_msa',\n",
    "        \"mean_sim_docvecs_pc\", \"mean_sim_ldavecs_pc\", \"mean_sim_docvecs_pc_msa\", \"mean_sim_ldavecs_pc_msa\",\n",
    "        \"sd_sim_docvecs_pc\", \"sd_sim_ldavecs_pc\", \"sd_sim_docvecs_pc_msa\", \"sd_sim_ldavecs_pc_msa\"]]\n",
    "fastparquet.write(\"DataStore/2018-10/pc_sim_pairs_0929.parq\", ps, compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
